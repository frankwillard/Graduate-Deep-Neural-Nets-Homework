{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhDUFBxt9xZg"
      },
      "source": [
        "# Implement and train a LSTM for sentiment analysis\n",
        "\n",
        "(General Hint on Lab 1/2: Trust whatever you see from the training and report it on PDF. IDMB is far from ideal as it's more like a real-world dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW6ymxu99xZk"
      },
      "source": [
        "## Step 0: set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spc_UH4B9xZl",
        "outputId": "e97899bf-cb80-4986-fb61-df9e757371ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "import os\n",
        "os.makedirs(\"resources\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvfWj66cKxDX"
      },
      "source": [
        "### Hyperparameters. Do not directly touch this to mess up settings.\n",
        "\n",
        "If you want to initalize new hyperparameter sets, use \"new_hparams = HyperParams()\" and change corresponding fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OxnFjs3f9xZn"
      },
      "outputs": [],
      "source": [
        "class HyperParams:\n",
        "    def __init__(self):\n",
        "        # Constance hyperparameters. They have been tested and don't need to be tuned.\n",
        "        self.PAD_INDEX = 0\n",
        "        self.UNK_INDEX = 1\n",
        "        self.PAD_TOKEN = '<pad>'\n",
        "        self.UNK_TOKEN = '<unk>'\n",
        "        self.STOP_WORDS = set(stopwords.words('english'))\n",
        "        self.MAX_LENGTH = 256\n",
        "        self.BATCH_SIZE = 96\n",
        "        self.EMBEDDING_DIM = 1\n",
        "        self.HIDDEN_DIM = 100\n",
        "        self.OUTPUT_DIM = 2\n",
        "        self.N_LAYERS = 1\n",
        "        self.DROPOUT_RATE = 0.0\n",
        "        self.LR = 0.01\n",
        "        self.N_EPOCHS = 5\n",
        "        self.WD = 0\n",
        "        self.OPTIM = \"sgd\"\n",
        "        self.BIDIRECTIONAL = False\n",
        "        self.SEED = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XODz_aDV9xZo"
      },
      "source": [
        "## Lab 1(a) Implement your own data loader function.  \n",
        "First, you need to read the data from the dataset file on the local disk. \n",
        "Then, split the dataset into three sets: train, validation and test by 7:1:2 ratio.\n",
        "Finally return x_train, x_valid, x_test, y_train, y_valid, y_test where x represents reviews and y represent labels.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AD7HSvM19xZp"
      },
      "outputs": [],
      "source": [
        "def load_imdb(base_csv:str = './IMDBDataset.csv'):\n",
        "    \"\"\"\n",
        "    Load the IMDB dataset\n",
        "    :param base_csv: the path of the dataset file.\n",
        "    :return: train, validation and test set.\n",
        "    \"\"\"\n",
        "    # Add your code here. \n",
        "\n",
        "    imdb_data = pd.read_csv(base_csv)\n",
        "    X = imdb_data.iloc[:, 0]\n",
        "    y = imdb_data.iloc[:, 1]\n",
        "\n",
        "    x_not_test, x_test, y_not_test, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "    \n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x_not_test, y_not_test, test_size = 0.125, random_state = 0)\n",
        "    \n",
        "    \n",
        "    print(f'shape of train data is {x_train.shape}')\n",
        "    print(f'shape of test data is {x_test.shape}')\n",
        "    print(f'shape of valid data is {x_valid.shape}')\n",
        "    return x_train, x_valid, x_test, y_train, y_valid, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYVH6t--9xZq"
      },
      "source": [
        "## Lab 1(b): Implement your function to build a vocabulary based on the training corpus.\n",
        "Implement the build_vocab function to build a vocabulary based on the training corpus.\n",
        "You should first compute the frequency of all the words in the training corpus. Remove the words\n",
        "that are in the STOP_WORDS. Then filter the words by their frequency (â‰¥ min_freq) and finally\n",
        "generate a corpus variable that contains a list of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sugI5VoJ9xZr"
      },
      "outputs": [],
      "source": [
        "def build_vocab(x_train:list, min_freq: int=5, hparams=None) -> dict:\n",
        "    \"\"\"\n",
        "    build a vocabulary based on the training corpus.\n",
        "    :param x_train:  List. The training corpus. Each sample in the list is a string of text.\n",
        "    :param min_freq: Int. The frequency threshold for selecting words.\n",
        "    :return: dictionary {word:index}\n",
        "    \"\"\"\n",
        "    # Add your code here. Your code should assign corpus with a list of words.\n",
        "\n",
        "    params = HyperParams()\n",
        "    stop_words = params.STOP_WORDS\n",
        "\n",
        "    corpus = {}\n",
        "    for review in x_train:\n",
        "      #Should I be removing neighboring punctuation (parentheses, br, comma, semicolon)\n",
        "      review_words = review.split(' ')\n",
        "\n",
        "      for current_word in review_words:\n",
        "        corpus[current_word] = corpus.get(current_word, 0) + 1\n",
        "\n",
        "    for stop_word in stop_words:\n",
        "      if stop_word in corpus:\n",
        "        corpus.pop(stop_word)\n",
        "    \n",
        "    # sorting on the basis of most common words\n",
        "    # corpus_ = sorted(corpus, key=corpus.get, reverse=True)[:1000]\n",
        "    corpus_ = [word for word, freq in corpus.items() if freq >= min_freq]\n",
        "    # creating a dict\n",
        "    vocab = {w:i+2 for i, w in enumerate(corpus_)}\n",
        "    vocab[hparams.PAD_TOKEN] = hparams.PAD_INDEX\n",
        "    vocab[hparams.UNK_TOKEN] = hparams.UNK_INDEX\n",
        "    return vocab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca71G17F9xZt"
      },
      "source": [
        "## Lab 1(c): Implement your tokenize function. \n",
        "For each word, find its index in the vocabulary. \n",
        "Return a list of int that represents the indices of words in the example. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c6kj_qT69xZt"
      },
      "outputs": [],
      "source": [
        "def tokenize(vocab: dict, example: str)-> list:\n",
        "    \"\"\"\n",
        "    Tokenize the give example string into a list of token indices.\n",
        "    :param vocab: dict, the vocabulary.\n",
        "    :param example: a string of text.\n",
        "    :return: a list of token indices.\n",
        "    \"\"\"\n",
        "    # Your code here.\n",
        "    return [vocab[current_word] for current_word in example.split(\" \") if current_word in vocab]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9ntSo4k9xZu"
      },
      "source": [
        "## Lab 1 (d): Implement the __getitem__ function. Given an index i, you should return the i-th review and label. \n",
        "The review is originally a string. Please tokenize it into a sequence of token indices. \n",
        "Use the max_length parameter to truncate the sequence so that it contains at most max_length tokens. \n",
        "Convert the label string ('positive'/'negative') to a binary index. 'positive' is 1 and 'negative' is 0. \n",
        "Return a dictionary containing three keys: 'ids', 'length', 'label' which represent the list of token ids, the length of the sequence, the binary label. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2TDgA4p79xZu"
      },
      "outputs": [],
      "source": [
        "class IMDB(Dataset):\n",
        "    def __init__(self, x, y, vocab, max_length=256) -> None:\n",
        "        \"\"\"\n",
        "        :param x: list of reviews\n",
        "        :param y: list of labels\n",
        "        :param vocab: vocabulary dictionary {word:index}.\n",
        "        :param max_length: the maximum sequence length.\n",
        "        \"\"\"\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.vocab = vocab\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        \"\"\"\n",
        "        Return the tokenized review and label by the given index.\n",
        "        :param idx: index of the sample.\n",
        "        :return: a dictionary containing three keys: 'ids', 'length', 'label' which represent the list of token ids, the length of the sequence, the binary label.\n",
        "        \"\"\"\n",
        "        # Add your code here.\n",
        "\n",
        "        current_review = self.x.iloc[idx]\n",
        "\n",
        "        my_item = {}\n",
        "        my_item['ids'] = tokenize(self.vocab, current_review)[:self.max_length]\n",
        "        my_item['length'] = len(my_item['ids'])\n",
        "        my_item['label'] = int(self.y.iloc[idx] == 'positive')\n",
        "        \n",
        "        return my_item\n",
        "    \n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.x)\n",
        "\n",
        "def collate(batch, pad_index):\n",
        "    batch_ids = [torch.LongTensor(i['ids']) for i in batch]\n",
        "    batch_ids = nn.utils.rnn.pad_sequence(batch_ids, padding_value=pad_index, batch_first=True)\n",
        "    batch_length = torch.Tensor([i['length'] for i in batch])\n",
        "    batch_label = torch.LongTensor([i['label'] for i in batch])\n",
        "    batch = {'ids': batch_ids, 'length': batch_length, 'label': batch_label}\n",
        "    return batch\n",
        "\n",
        "collate_fn = collate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zgSPYmf9xZv"
      },
      "source": [
        "## Lab 1 (e): Implement the LSTM model for sentiment analysis.\n",
        "Q(a): Implement the initialization function.\n",
        "Your task is to create the model by stacking several necessary layers including an embedding layer, a lstm cell, a linear layer, and a dropout layer.\n",
        "You can call functions from Pytorch's nn library. For example, nn.Embedding, nn.LSTM, nn.Linear.<br>\n",
        "Q(b): Implement the forward function.\n",
        "    Decide where to apply dropout. \n",
        "    The sequences in the batch have different lengths. Write/call a function to pad the sequences into the same length. \n",
        "    Apply a fully-connected (fc) layer to the output of the LSTM layer. \n",
        "    Return the output features which is of size [batch size, output dim]. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dlfuiQwcebWI"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "b9ofQ5R29xZv"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Embedding):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.LSTM) or isinstance(m, nn.GRU):\n",
        "        for name, param in m.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "            elif 'weight' in name:\n",
        "                nn.init.orthogonal_(param)\n",
        "                \n",
        "class LSTM(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        vocab_size: int, \n",
        "        embedding_dim: int, \n",
        "        hidden_dim: int, \n",
        "        output_dim: int, \n",
        "        n_layers: int, \n",
        "        dropout_rate: float, \n",
        "        pad_index: int,\n",
        "        bidirectional: bool,\n",
        "        **kwargs):\n",
        "        \"\"\"\n",
        "        Create a LSTM model for classification.\n",
        "        :param vocab_size: size of the vocabulary\n",
        "        :param embedding_dim: dimension of embeddings\n",
        "        :param hidden_dim: dimension of hidden features\n",
        "        :param output_dim: dimension of the output layer which equals to the number of labels.\n",
        "        :param n_layers: number of layers.\n",
        "        :param dropout_rate: dropout rate.\n",
        "        :param pad_index: index of the padding token.we\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Add your code here. Initializing each layer by the given arguments.\n",
        "        \n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.pad_index = pad_index\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self.emb1 = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx = self.pad_index)\n",
        "        self.pack_padded = pack_padded_sequence\n",
        "        self.lstm_cell1 = nn.LSTM(self.embedding_dim, self.hidden_dim,\n",
        "                                  self.n_layers, bidirectional = self.bidirectional)\n",
        "        self.fc1 = nn.Linear(self.hidden_dim, self.output_dim)\n",
        "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
        "        \n",
        "        # Weight initialization. DO NOT CHANGE!\n",
        "        if \"weight_init_fn\" not in kwargs:\n",
        "            self.apply(init_weights)\n",
        "        else:\n",
        "            self.apply(kwargs[\"weight_init_fn\"])\n",
        "\n",
        "\n",
        "    def forward(self, ids:torch.Tensor, length:torch.Tensor):\n",
        "        \"\"\"\n",
        "        Feed the given token ids to the model.\n",
        "        :param ids: [batch size, seq len] batch of token ids.\n",
        "        :param length: [batch size] batch of length of the token ids.\n",
        "        :return: prediction of size [batch size, output dim].\n",
        "        \"\"\"\n",
        "        lstm_ids = self.pack_padded(self.emb1(ids), length, batch_first = True, enforce_sorted=False)\n",
        "        \n",
        "        #features, hidden state, cell state\n",
        "        output, (h_n, c_n) = self.lstm_cell1(lstm_ids)\n",
        "        prediction = self.fc1(self.dropout(h_n[-1]))\n",
        "        \n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "13Sdl7MV9xZv"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def train(dataloader, model, criterion, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "    epoch_accs = []\n",
        "\n",
        "    for batch in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):\n",
        "        ids = batch['ids'].to(device)\n",
        "        length = batch['length']\n",
        "        label = batch['label'].to(device)\n",
        "        prediction = model(ids, length)\n",
        "        loss = criterion(prediction, label)\n",
        "        accuracy = get_accuracy(prediction, label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_losses.append(loss.item())\n",
        "        epoch_accs.append(accuracy.item())\n",
        "        scheduler.step()\n",
        "\n",
        "    return epoch_losses, epoch_accs\n",
        "\n",
        "def evaluate(dataloader, model, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_losses = []\n",
        "    epoch_accs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(dataloader, desc='evaluating...', file=sys.stdout):\n",
        "            ids = batch['ids'].to(device)\n",
        "            length = batch['length']\n",
        "            label = batch['label'].to(device)\n",
        "            prediction = model(ids, length)\n",
        "            loss = criterion(prediction, label)\n",
        "            accuracy = get_accuracy(prediction, label)\n",
        "            epoch_losses.append(loss.item())\n",
        "            epoch_accs.append(accuracy.item())\n",
        "\n",
        "    return epoch_losses, epoch_accs\n",
        "\n",
        "def get_accuracy(prediction, label):\n",
        "    batch_size, _ = prediction.shape\n",
        "    predicted_classes = prediction.argmax(dim=-1)\n",
        "    correct_predictions = predicted_classes.eq(label).sum()\n",
        "    accuracy = correct_predictions / batch_size\n",
        "    return accuracy\n",
        "\n",
        "def predict_sentiment(text, model, vocab, device):\n",
        "    tokens = tokenize(vocab, text)\n",
        "    ids = [vocab[t] if t in vocab else UNK_INDEX for t in tokens]\n",
        "    length = torch.LongTensor([len(ids)])\n",
        "    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)\n",
        "    prediction = model(tensor, length).squeeze(dim=0)\n",
        "    probability = torch.softmax(prediction, dim=-1)\n",
        "    predicted_class = prediction.argmax(dim=-1).item()\n",
        "    predicted_probability = probability[predicted_class].item()\n",
        "    return predicted_class, predicted_probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W-U2fRFKxDg"
      },
      "source": [
        "### Lab 1 (g) Implement GRU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EqpvKcpfKxDh"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        vocab_size: int, \n",
        "        embedding_dim: int, \n",
        "        hidden_dim: int, \n",
        "        output_dim: int, \n",
        "        n_layers: int, \n",
        "        dropout_rate: float, \n",
        "        pad_index: int,\n",
        "        bidirectional: bool,\n",
        "        **kwargs):\n",
        "        \"\"\"\n",
        "        Create a LSTM model for classification.\n",
        "        :param vocab_size: size of the vocabulary\n",
        "        :param embedding_dim: dimension of embeddings\n",
        "        :param hidden_dim: dimension of hidden features\n",
        "        :param output_dim: dimension of the output layer which equals to the number of labels.\n",
        "        :param n_layers: number of layers.\n",
        "        :param dropout_rate: dropout rate.\n",
        "        :param pad_index: index of the padding token.we\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Add your code here. Initializing each layer by the given arguments.\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.pad_index = pad_index\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self.pack_padded = pack_padded_sequence\n",
        "        self.emb1 = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx = self.pad_index)\n",
        "        self.gru_cell1 = nn.GRU(self.embedding_dim, self.hidden_dim, self.n_layers,\n",
        "                                 bidirectional = self.bidirectional)\n",
        "        self.fc1 = nn.Linear(self.hidden_dim, self.output_dim)\n",
        "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
        "\n",
        "        # Weight Initialization. DO NOT CHANGE!\n",
        "        if \"weight_init_fn\" not in kwargs:\n",
        "            self.apply(init_weights)\n",
        "        else:\n",
        "            self.apply(kwargs[\"weight_init_fn\"])\n",
        "\n",
        "\n",
        "    def forward(self, ids:torch.Tensor, length:torch.Tensor):\n",
        "        \"\"\"\n",
        "        Feed the given token ids to the model.\n",
        "        :param ids: [batch size, seq len] batch of token ids.\n",
        "        :param length: [batch size] batch of length of the token ids.\n",
        "        :return: prediction of size [batch size, output dim].\n",
        "        \"\"\"\n",
        "        # Add your code here.\n",
        "        gru_ids = self.pack_padded(self.emb1(ids), length, batch_first = True, enforce_sorted=False)\n",
        "        \n",
        "        #features, hidden state, cell state\n",
        "        output, h_n = self.gru_cell1(gru_ids)\n",
        "        prediction = self.fc1(self.dropout(h_n[-1]))\n",
        "        \n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndKco89DKxDh"
      },
      "source": [
        "### Learning rate warmup. DO NOT TOUCH!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9YhUvsnoKxDi"
      },
      "outputs": [],
      "source": [
        "class ConstantWithWarmup(torch.optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(\n",
        "        self,\n",
        "        optimizer,\n",
        "        num_warmup_steps: int,\n",
        "    ):\n",
        "        self.num_warmup_steps = num_warmup_steps\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self._step_count <= self.num_warmup_steps:\n",
        "            # warmup\n",
        "            scale = 1.0 - (self.num_warmup_steps - self._step_count) / self.num_warmup_steps\n",
        "            lr = [base_lr * scale for base_lr in self.base_lrs]\n",
        "            self.last_lr = lr\n",
        "        else:\n",
        "            lr = self.base_lrs\n",
        "        return lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8BZJUMKKxDi"
      },
      "source": [
        "### Implement the training / validation iteration here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qXLkQSnS9xZw"
      },
      "outputs": [],
      "source": [
        "def train_and_test_model_with_hparams(hparams, model_type=\"lstm\", **kwargs):\n",
        "    # Seeding. DO NOT TOUCH! DO NOT TOUCH hparams.SEED!\n",
        "    # Set the random seeds.\n",
        "    torch.manual_seed(hparams.SEED)\n",
        "    random.seed(hparams.SEED)\n",
        "    np.random.seed(hparams.SEED)\n",
        "\n",
        "    x_train, x_valid, x_test, y_train, y_valid, y_test = load_imdb()\n",
        "    vocab = build_vocab(x_train, hparams=hparams)\n",
        "    vocab_size = len(vocab)\n",
        "    print(f'Length of vocabulary is {vocab_size}')\n",
        "\n",
        "    train_data = IMDB(x_train, y_train, vocab, hparams.MAX_LENGTH)\n",
        "    valid_data = IMDB(x_valid, y_valid, vocab, hparams.MAX_LENGTH)\n",
        "    test_data = IMDB(x_test, y_test, vocab, hparams.MAX_LENGTH)\n",
        "\n",
        "    collate = functools.partial(collate_fn, pad_index=hparams.PAD_INDEX)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_data, batch_size=hparams.BATCH_SIZE, collate_fn=collate, shuffle=True)\n",
        "    valid_dataloader = torch.utils.data.DataLoader(\n",
        "        valid_data, batch_size=hparams.BATCH_SIZE, collate_fn=collate)\n",
        "    test_dataloader = torch.utils.data.DataLoader(\n",
        "        test_data, batch_size=hparams.BATCH_SIZE, collate_fn=collate)\n",
        "    \n",
        "    # Model\n",
        "    if \"override_models_with_gru\" in kwargs and kwargs[\"override_models_with_gru\"]:\n",
        "        model = GRU(\n",
        "            vocab_size, \n",
        "            hparams.EMBEDDING_DIM, \n",
        "            hparams.HIDDEN_DIM, \n",
        "            hparams.OUTPUT_DIM,\n",
        "            hparams.N_LAYERS,\n",
        "            hparams.DROPOUT_RATE, \n",
        "            hparams.PAD_INDEX,\n",
        "            hparams.BIDIRECTIONAL,\n",
        "            **kwargs)\n",
        "    else:\n",
        "        model = LSTM(\n",
        "            vocab_size, \n",
        "            hparams.EMBEDDING_DIM, \n",
        "            hparams.HIDDEN_DIM, \n",
        "            hparams.OUTPUT_DIM,\n",
        "            hparams.N_LAYERS,\n",
        "            hparams.DROPOUT_RATE, \n",
        "            hparams.PAD_INDEX,\n",
        "            hparams.BIDIRECTIONAL,\n",
        "            **kwargs)\n",
        "    num_params = count_parameters(model)\n",
        "    print(f'The model has {num_params:,} trainable parameters')\n",
        "\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Optimization. Lab 2 (a)(b) should choose one of them.\n",
        "    # DO NOT TOUCH optimizer-specific hyperparameters! (e.g., eps, momentum)\n",
        "    # DO NOT change optimizer implementations!\n",
        "    if hparams.OPTIM == \"sgd\":\n",
        "        optimizer = optim.SGD(\n",
        "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, momentum=.9)        \n",
        "    elif hparams.OPTIM == \"adagrad\":\n",
        "        optimizer = optim.Adagrad(\n",
        "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6)\n",
        "    elif hparams.OPTIM == \"adam\":\n",
        "        optimizer = optim.Adam(\n",
        "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6)\n",
        "    elif hparams.OPTIM == \"rmsprop\":\n",
        "        optimizer = optim.RMSprop(\n",
        "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6, momentum=.9)\n",
        "    else:\n",
        "        raise NotImplementedError(\"Optimizer not implemented!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    # Start training\n",
        "    best_valid_loss = float('inf')\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "    \n",
        "    # Warmup Scheduler. DO NOT TOUCH!\n",
        "    WARMUP_STEPS = 200\n",
        "    lr_scheduler = ConstantWithWarmup(optimizer, WARMUP_STEPS)\n",
        "\n",
        "    best_path = 'BestModel.pth'\n",
        "\n",
        "    for epoch in range(hparams.N_EPOCHS):\n",
        "        \n",
        "        # Your code: implement the training process and save the best model.\n",
        "        \n",
        "        train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, lr_scheduler, device)\n",
        "        valid_loss, valid_acc = evaluate(valid_dataloader, model, criterion, device)\n",
        "        \n",
        "        \n",
        "        epoch_train_loss = np.mean(train_loss)\n",
        "        epoch_train_acc = np.mean(train_acc)\n",
        "        epoch_valid_loss = np.mean(valid_loss)\n",
        "        epoch_valid_acc = np.mean(valid_acc)\n",
        "\n",
        "        # Save the model that achieves the smallest validation loss.\n",
        "        if epoch_valid_loss < best_valid_loss:\n",
        "            # Your code: save the best model somewhere (no need to submit it to Sakai)\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "\n",
        "        print(f'epoch: {epoch+1}')\n",
        "        print(f'train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}')\n",
        "        print(f'valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}')\n",
        "\n",
        "\n",
        "    # Your Code: Load the best model's weights.\n",
        "    best_state_dict = torch.load(best_path)\n",
        "    model.load_state_dict(best_state_dict)\n",
        "\n",
        "    # Your Code: evaluate test loss on testing dataset (NOT Validation)\n",
        "    test_loss, test_acc = evaluate(test_dataloader, model, criterion, device)\n",
        "\n",
        "    epoch_test_loss = np.mean(test_loss)\n",
        "    epoch_test_acc = np.mean(test_acc)\n",
        "    print(f'test_loss: {epoch_test_loss:.3f}, test_acc: {epoch_test_acc:.3f}')\n",
        "    \n",
        "    # Free memory for later usage.\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    return {\n",
        "        'num_params': num_params,\n",
        "        \"test_loss\": epoch_test_loss,\n",
        "        \"test_acc\": epoch_test_acc,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKCu4rPBA2Sp"
      },
      "source": [
        "### Lab 1 (f): Train model with original hyperparameters, for LSTM.\n",
        "\n",
        "Train the model with default hyperparameter settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0EhpiZhKxDj",
        "outputId": "9e6fc4d0-ec45-40e2-bdb5-b182550bc65b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 102,235 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 40.61it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 91.42it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.496\n",
            "valid_loss: 0.694, valid_acc: 0.496\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.61it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 89.39it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.693, train_acc: 0.498\n",
            "valid_loss: 0.693, valid_acc: 0.504\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 52.61it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 92.07it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.693, train_acc: 0.501\n",
            "valid_loss: 0.693, valid_acc: 0.496\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.85it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.51it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.693, train_acc: 0.500\n",
            "valid_loss: 0.694, valid_acc: 0.496\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.77it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 91.78it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.694, train_acc: 0.496\n",
            "valid_loss: 0.694, valid_acc: 0.496\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 86.75it/s]\n",
            "test_loss: 0.694, test_acc: 0.498\n"
          ]
        }
      ],
      "source": [
        "org_hyperparams = HyperParams()\n",
        "_ = train_and_test_model_with_hparams(org_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36O6qdy1KxDj"
      },
      "source": [
        "### Lab 1 (h) Train GRU with vanilla hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm65UH3NKxDj",
        "outputId": "ff7e1f6f-7d01-43c4-9a4f-d4872d2784d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 91,935 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 53.84it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 91.43it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.694, train_acc: 0.498\n",
            "valid_loss: 0.694, valid_acc: 0.496\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 52.24it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 28.47it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.694, train_acc: 0.496\n",
            "valid_loss: 0.696, valid_acc: 0.504\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.40it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 91.70it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.694, train_acc: 0.499\n",
            "valid_loss: 0.694, valid_acc: 0.496\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 55.00it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 93.13it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.694, train_acc: 0.496\n",
            "valid_loss: 0.696, valid_acc: 0.496\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 54.73it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 91.48it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.694, train_acc: 0.501\n",
            "valid_loss: 0.695, valid_acc: 0.496\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 92.59it/s]\n",
            "test_loss: 0.695, test_acc: 0.498\n"
          ]
        }
      ],
      "source": [
        "org_hyperparams = HyperParams()\n",
        "_ = train_and_test_model_with_hparams(org_hyperparams, \"gru_1layer_base_sgd_e32_h100\", override_models_with_gru=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ifx6jR7KxDk"
      },
      "source": [
        "### Lab 2 (a) Study of LSTM Optimizers. Hint: For adaptive optimizers, we recommend using a learning rate of 0.001 (instead of 0.01)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2C7zY4Hnp65",
        "outputId": "d125e3b3-30e5-4d00-fdea-2114ca38a727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 102,235 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.22it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 90.41it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.504\n",
            "valid_loss: 0.693, valid_acc: 0.496\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 52.58it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 92.89it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.666, train_acc: 0.618\n",
            "valid_loss: 0.573, valid_acc: 0.809\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 52.97it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 94.40it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.548, train_acc: 0.810\n",
            "valid_loss: 0.537, valid_acc: 0.814\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.16it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 89.44it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.502, train_acc: 0.831\n",
            "valid_loss: 0.566, valid_acc: 0.783\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.44it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 92.78it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.473, train_acc: 0.846\n",
            "valid_loss: 0.515, valid_acc: 0.819\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 89.69it/s]\n",
            "test_loss: 0.538, test_acc: 0.808\n"
          ]
        }
      ],
      "source": [
        "adagrad_optimizer_hyperparams = HyperParams()\n",
        "adagrad_optimizer_hyperparams.OPTIM = \"adagrad\"\n",
        "adagrad_optimizer_hyperparams.LR = 0.001\n",
        "_ = train_and_test_model_with_hparams(adagrad_optimizer_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CZsoa7IKxDk",
        "outputId": "699fd9cf-63ef-4035-d416-e29b9242cce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 102,235 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.24it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.04it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.665, train_acc: 0.578\n",
            "valid_loss: 0.610, valid_acc: 0.688\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.06it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 91.10it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.330, train_acc: 0.864\n",
            "valid_loss: 0.309, valid_acc: 0.884\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.15it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.45it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.144, train_acc: 0.950\n",
            "valid_loss: 0.306, valid_acc: 0.887\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.23it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 90.53it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.076, train_acc: 0.976\n",
            "valid_loss: 0.339, valid_acc: 0.869\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.89it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.72it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.042, train_acc: 0.988\n",
            "valid_loss: 0.464, valid_acc: 0.881\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 90.07it/s]\n",
            "test_loss: 0.507, test_acc: 0.868\n"
          ]
        }
      ],
      "source": [
        "adam_optimizer_hyperparams = HyperParams()\n",
        "adam_optimizer_hyperparams.OPTIM = \"adam\"\n",
        "adam_optimizer_hyperparams.LR = 0.001\n",
        "_ = train_and_test_model_with_hparams(adam_optimizer_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5DMW_FXnvN2",
        "outputId": "f4d5bee5-5a3e-45b3-fd46-539036d51906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 102,235 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.91it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 91.81it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.554, train_acc: 0.706\n",
            "valid_loss: 0.398, valid_acc: 0.835\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.59it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 90.75it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.280, train_acc: 0.893\n",
            "valid_loss: 0.301, valid_acc: 0.881\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 45.83it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.30it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.147, train_acc: 0.949\n",
            "valid_loss: 0.441, valid_acc: 0.859\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.69it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 93.29it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.079, train_acc: 0.974\n",
            "valid_loss: 0.428, valid_acc: 0.861\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.85it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 89.30it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.042, train_acc: 0.987\n",
            "valid_loss: 0.506, valid_acc: 0.863\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 89.99it/s]\n",
            "test_loss: 0.522, test_acc: 0.857\n"
          ]
        }
      ],
      "source": [
        "rmsprop_optimizer_hyperparams = HyperParams()\n",
        "rmsprop_optimizer_hyperparams.OPTIM = \"rmsprop\"\n",
        "rmsprop_optimizer_hyperparams.LR = 0.001\n",
        "_ = train_and_test_model_with_hparams(rmsprop_optimizer_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN9b59ziKxDl"
      },
      "source": [
        "### Lab 2 (b): Study of GRU Optimizers. Hint: For adaptive optimizers, we recommend using a learning rate of 0.001 (instead of 0.01)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS5R-hmmKxDl",
        "outputId": "ccd53c0c-352c-4aa8-d851-813891be5ef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 91,935 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.94it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.22it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.514\n",
            "valid_loss: 0.692, valid_acc: 0.584\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.29it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.30it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.613, train_acc: 0.721\n",
            "valid_loss: 0.562, valid_acc: 0.770\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 52.72it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 91.68it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.464, train_acc: 0.854\n",
            "valid_loss: 0.465, valid_acc: 0.848\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 52.84it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 90.38it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.398, train_acc: 0.879\n",
            "valid_loss: 0.450, valid_acc: 0.853\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 52.89it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.59it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.353, train_acc: 0.895\n",
            "valid_loss: 0.403, valid_acc: 0.868\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 87.73it/s]\n",
            "test_loss: 0.411, test_acc: 0.854\n"
          ]
        }
      ],
      "source": [
        "_ = train_and_test_model_with_hparams(adagrad_optimizer_hyperparams, \"gru_1layer_base_sgd_e32_h100\", override_models_with_gru=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VovsiFceoBBy",
        "outputId": "db881d80-e3e9-4169-de91-526cb9c67512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 91,935 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.76it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 91.69it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.647, train_acc: 0.610\n",
            "valid_loss: 0.466, valid_acc: 0.814\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 52.08it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 91.64it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.252, train_acc: 0.901\n",
            "valid_loss: 0.293, valid_acc: 0.877\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.32it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 91.17it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.112, train_acc: 0.963\n",
            "valid_loss: 0.326, valid_acc: 0.888\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.60it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 90.68it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.048, train_acc: 0.987\n",
            "valid_loss: 0.398, valid_acc: 0.884\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 52.93it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 92.68it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.038, train_acc: 0.989\n",
            "valid_loss: 0.592, valid_acc: 0.865\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 89.57it/s]\n",
            "test_loss: 0.639, test_acc: 0.858\n"
          ]
        }
      ],
      "source": [
        "_ = train_and_test_model_with_hparams(adam_optimizer_hyperparams, \"gru_1layer_base_sgd_e32_h100\", override_models_with_gru=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ1BbEseoBhK",
        "outputId": "9303c4d8-14e6-4732-d1fc-ac41e09999e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 91,935 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.72it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 90.13it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.536, train_acc: 0.711\n",
            "valid_loss: 0.283, valid_acc: 0.881\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 53.19it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 89.98it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.188, train_acc: 0.926\n",
            "valid_loss: 0.274, valid_acc: 0.883\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 47.08it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 92.48it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.079, train_acc: 0.972\n",
            "valid_loss: 0.376, valid_acc: 0.873\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.50it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.58it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.033, train_acc: 0.990\n",
            "valid_loss: 0.476, valid_acc: 0.865\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 49.80it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.61it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.021, train_acc: 0.993\n",
            "valid_loss: 0.568, valid_acc: 0.873\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 83.84it/s]\n",
            "test_loss: 0.624, test_acc: 0.864\n"
          ]
        }
      ],
      "source": [
        "_ = train_and_test_model_with_hparams(rmsprop_optimizer_hyperparams, \"gru_1layer_base_sgd_e32_h100\", override_models_with_gru=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8emVsT9BKxDl"
      },
      "source": [
        "### Lab 2 (c) Deeper LSTMs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam_depth_1_hyperparams = HyperParams()\n",
        "adam_depth_1_hyperparams.OPTIM = \"adam\"\n",
        "adam_depth_1_hyperparams.LR = 0.001\n",
        "adam_depth_1_hyperparams.N_LAYERS = 1"
      ],
      "metadata": {
        "id": "wVT8Khw0wP1r"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdVG1amhKxDl",
        "outputId": "4bb8f5de-c5be-4786-ded7-2ff0cd915566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 183,035 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 32.57it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 64.54it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.683, train_acc: 0.573\n",
            "valid_loss: 0.561, valid_acc: 0.753\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 30.95it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 65.54it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.343, train_acc: 0.855\n",
            "valid_loss: 0.304, valid_acc: 0.876\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 32.66it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 68.51it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.168, train_acc: 0.939\n",
            "valid_loss: 0.308, valid_acc: 0.881\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 32.61it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 63.85it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.087, train_acc: 0.972\n",
            "valid_loss: 0.343, valid_acc: 0.872\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 33.17it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 70.76it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.045, train_acc: 0.987\n",
            "valid_loss: 0.481, valid_acc: 0.858\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 67.59it/s]\n",
            "test_loss: 0.512, test_acc: 0.852\n"
          ]
        }
      ],
      "source": [
        "adam_depth_2_hyperparams = HyperParams()\n",
        "adam_depth_2_hyperparams.OPTIM = \"adam\"\n",
        "adam_depth_2_hyperparams.LR = 0.001\n",
        "adam_depth_2_hyperparams.N_LAYERS = 2\n",
        "_ = train_and_test_model_with_hparams(adam_depth_2_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOiyRSOQpRaY",
        "outputId": "4167d6a3-2577-4d3e-ca77-6f3654b58be7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 263,835 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 25.36it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 57.86it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.671, train_acc: 0.570\n",
            "valid_loss: 0.376, valid_acc: 0.839\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:15<00:00, 24.23it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 56.96it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.294, train_acc: 0.881\n",
            "valid_loss: 0.324, valid_acc: 0.871\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 25.56it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 57.69it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.146, train_acc: 0.949\n",
            "valid_loss: 0.318, valid_acc: 0.886\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 25.92it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 57.02it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.075, train_acc: 0.977\n",
            "valid_loss: 0.419, valid_acc: 0.883\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 25.58it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 57.02it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.039, train_acc: 0.989\n",
            "valid_loss: 0.462, valid_acc: 0.866\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 56.09it/s]\n",
            "test_loss: 0.510, test_acc: 0.852\n"
          ]
        }
      ],
      "source": [
        "adam_depth_3_hyperparams = HyperParams()\n",
        "adam_depth_3_hyperparams.OPTIM = \"adam\"\n",
        "adam_depth_3_hyperparams.LR = 0.001\n",
        "adam_depth_3_hyperparams.N_LAYERS = 3\n",
        "_ = train_and_test_model_with_hparams(adam_depth_3_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6SNIW_jpRxj",
        "outputId": "5ddc6200-a030-432f-bc7e-bfbe00331270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 344,635 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:17<00:00, 21.13it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 51.22it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.503\n",
            "valid_loss: 0.693, valid_acc: 0.504\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:17<00:00, 20.84it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 48.96it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.593, train_acc: 0.648\n",
            "valid_loss: 0.449, valid_acc: 0.815\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:17<00:00, 21.07it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 48.04it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.265, train_acc: 0.897\n",
            "valid_loss: 0.296, valid_acc: 0.877\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:17<00:00, 21.08it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 32.93it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.131, train_acc: 0.955\n",
            "valid_loss: 0.326, valid_acc: 0.877\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:17<00:00, 20.86it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 49.21it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.066, train_acc: 0.981\n",
            "valid_loss: 0.387, valid_acc: 0.869\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:02<00:00, 49.04it/s]\n",
            "test_loss: 0.419, test_acc: 0.864\n"
          ]
        }
      ],
      "source": [
        "adam_depth_4_hyperparams = HyperParams()\n",
        "adam_depth_4_hyperparams.OPTIM = \"adam\"\n",
        "adam_depth_4_hyperparams.LR = 0.001\n",
        "adam_depth_4_hyperparams.N_LAYERS = 4\n",
        "_ = train_and_test_model_with_hparams(adam_depth_4_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEQwX2gHKxDm"
      },
      "source": [
        "### Lab 2 (d) Wider LSTMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJF3mh62t9ek",
        "outputId": "e4deb13f-157d-4a89-f171-fc3ecbead9a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 63,685 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 43.56it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.76it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.657, train_acc: 0.577\n",
            "valid_loss: 0.581, valid_acc: 0.763\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 47.41it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.48it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.297, train_acc: 0.882\n",
            "valid_loss: 0.271, valid_acc: 0.892\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 41.18it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 33.79it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.147, train_acc: 0.950\n",
            "valid_loss: 0.275, valid_acc: 0.891\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 45.91it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 55.59it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.079, train_acc: 0.976\n",
            "valid_loss: 0.383, valid_acc: 0.883\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 47.15it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.89it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.043, train_acc: 0.988\n",
            "valid_loss: 0.388, valid_acc: 0.877\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 67.20it/s]\n",
            "test_loss: 0.417, test_acc: 0.872\n"
          ]
        }
      ],
      "source": [
        "adam_width_25_hyperparams = HyperParams()\n",
        "adam_width_25_hyperparams.OPTIM = \"adam\"\n",
        "adam_width_25_hyperparams.LR = 0.001\n",
        "adam_width_25_hyperparams.N_LAYERS = 1\n",
        "#adam_width_25_hyperparams.DROPOUT_RATE = 0.5\n",
        "\n",
        "adam_width_25_hyperparams.HIDDEN_DIM = 25\n",
        "_ = train_and_test_model_with_hparams(adam_width_25_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg3ynppRqBcZ",
        "outputId": "c3426006-41d5-4865-a793-484bf0def45b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 71,535 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 45.50it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.14it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.621, train_acc: 0.609\n",
            "valid_loss: 0.332, valid_acc: 0.863\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 52.16it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.56it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.265, train_acc: 0.897\n",
            "valid_loss: 0.315, valid_acc: 0.870\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 42.75it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.57it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.133, train_acc: 0.954\n",
            "valid_loss: 0.288, valid_acc: 0.891\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.95it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 90.68it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.066, train_acc: 0.981\n",
            "valid_loss: 0.337, valid_acc: 0.883\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 41.75it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 47.38it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.059, train_acc: 0.980\n",
            "valid_loss: 0.405, valid_acc: 0.825\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:02<00:00, 51.61it/s]\n",
            "test_loss: 0.418, test_acc: 0.819\n"
          ]
        }
      ],
      "source": [
        "adam_width_50_hyperparams = HyperParams()\n",
        "adam_width_50_hyperparams.OPTIM = \"adam\"\n",
        "adam_width_50_hyperparams.LR = 0.001\n",
        "adam_width_50_hyperparams.N_LAYERS = 1\n",
        "#adam_width_50_hyperparams.DROPOUT_RATE = 0.5\n",
        "\n",
        "adam_width_50_hyperparams.HIDDEN_DIM = 50\n",
        "_ = train_and_test_model_with_hparams(adam_width_50_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh1gf82xuAck",
        "outputId": "360926e4-c7d8-4bb0-9e5a-edd649724786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 84,385 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.01it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 61.47it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.660, train_acc: 0.581\n",
            "valid_loss: 0.417, valid_acc: 0.833\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 49.25it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 83.64it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.280, train_acc: 0.891\n",
            "valid_loss: 0.281, valid_acc: 0.888\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.21it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.75it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.141, train_acc: 0.951\n",
            "valid_loss: 0.312, valid_acc: 0.870\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.96it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.91it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.076, train_acc: 0.977\n",
            "valid_loss: 0.341, valid_acc: 0.881\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.66it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.40it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.052, train_acc: 0.985\n",
            "valid_loss: 0.464, valid_acc: 0.867\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 84.59it/s]\n",
            "test_loss: 0.486, test_acc: 0.863\n"
          ]
        }
      ],
      "source": [
        "adam_width_75_hyperparams = HyperParams()\n",
        "adam_width_75_hyperparams.OPTIM = \"adam\"\n",
        "adam_width_75_hyperparams.LR = 0.001\n",
        "adam_width_75_hyperparams.N_LAYERS = 1\n",
        "#adam_width_75_hyperparams.DROPOUT_RATE = 0.5\n",
        "\n",
        "adam_width_75_hyperparams.HIDDEN_DIM = 75\n",
        "_ = train_and_test_model_with_hparams(adam_width_75_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam_width_100_hyperparams = HyperParams()\n",
        "adam_width_100_hyperparams.OPTIM = \"adam\"\n",
        "adam_width_100_hyperparams.LR = 0.001\n",
        "adam_width_100_hyperparams.N_LAYERS = 1\n",
        "#adam_width_100_hyperparams.DROPOUT_RATE = 0.5"
      ],
      "metadata": {
        "id": "Y7rxSWpAyjto"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lnr6TZaZuHUz",
        "outputId": "120a429c-09b1-47a6-b8a5-40f9db26026c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 125,085 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.00it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 83.21it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.677, train_acc: 0.561\n",
            "valid_loss: 0.621, valid_acc: 0.703\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 47.06it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 83.49it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.334, train_acc: 0.866\n",
            "valid_loss: 0.362, valid_acc: 0.849\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 45.87it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 77.77it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.163, train_acc: 0.942\n",
            "valid_loss: 0.344, valid_acc: 0.854\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 46.35it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.46it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.091, train_acc: 0.972\n",
            "valid_loss: 0.374, valid_acc: 0.874\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.23it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.50it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.049, train_acc: 0.986\n",
            "valid_loss: 0.441, valid_acc: 0.864\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 81.40it/s]\n",
            "test_loss: 0.474, test_acc: 0.864\n"
          ]
        }
      ],
      "source": [
        "adam_width_125_hyperparams = HyperParams()\n",
        "adam_width_125_hyperparams.OPTIM = \"adam\"\n",
        "adam_width_125_hyperparams.LR = 0.001\n",
        "adam_width_125_hyperparams.N_LAYERS = 1\n",
        "#adam_width_125_hyperparams.DROPOUT_RATE = 0.5\n",
        "\n",
        "adam_width_125_hyperparams.HIDDEN_DIM = 125\n",
        "_ = train_and_test_model_with_hparams(adam_width_125_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Paaj4rVuqGK_",
        "outputId": "880ef191-1c50-472d-ef7c-74a222d6a578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 152,935 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 43.57it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 82.49it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.681, train_acc: 0.573\n",
            "valid_loss: 0.543, valid_acc: 0.824\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 45.77it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 82.58it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.302, train_acc: 0.880\n",
            "valid_loss: 0.308, valid_acc: 0.869\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 42.02it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 83.00it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.153, train_acc: 0.948\n",
            "valid_loss: 0.293, valid_acc: 0.878\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 45.23it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 76.99it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.083, train_acc: 0.975\n",
            "valid_loss: 0.342, valid_acc: 0.880\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 45.30it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 80.85it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.044, train_acc: 0.988\n",
            "valid_loss: 0.406, valid_acc: 0.878\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 79.85it/s]\n",
            "test_loss: 0.432, test_acc: 0.872\n"
          ]
        }
      ],
      "source": [
        "adam_width_150_hyperparams = HyperParams()\n",
        "adam_width_150_hyperparams.OPTIM = \"adam\"\n",
        "adam_width_150_hyperparams.LR = 0.001\n",
        "adam_width_150_hyperparams.N_LAYERS = 1\n",
        "#adam_width_150_hyperparams.DROPOUT_RATE = 0.5\n",
        "\n",
        "adam_width_150_hyperparams.HIDDEN_DIM = 150\n",
        "_ = train_and_test_model_with_hparams(adam_width_150_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkFaLg6duMHA",
        "outputId": "bd5dd313-15b0-4893-d8e0-da76ad0dc19f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 185,785 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.04it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 78.57it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.676, train_acc: 0.563\n",
            "valid_loss: 0.633, valid_acc: 0.673\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.89it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 79.04it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.419, train_acc: 0.811\n",
            "valid_loss: 0.578, valid_acc: 0.696\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 45.39it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 79.32it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.236, train_acc: 0.910\n",
            "valid_loss: 0.296, valid_acc: 0.884\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.70it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 80.49it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.124, train_acc: 0.957\n",
            "valid_loss: 0.320, valid_acc: 0.878\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 42.92it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 75.70it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.067, train_acc: 0.980\n",
            "valid_loss: 0.395, valid_acc: 0.877\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 74.25it/s]\n",
            "test_loss: 0.432, test_acc: 0.874\n"
          ]
        }
      ],
      "source": [
        "adam_width_175_hyperparams = HyperParams()\n",
        "adam_width_175_hyperparams.OPTIM = \"adam\"\n",
        "adam_width_175_hyperparams.LR = 0.001\n",
        "adam_width_175_hyperparams.N_LAYERS = 1\n",
        "#adam_width_175_hyperparams.DROPOUT_RATE = 0.5\n",
        "\n",
        "adam_width_175_hyperparams.HIDDEN_DIM = 175\n",
        "_ = train_and_test_model_with_hparams(adam_width_175_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlQfPgU8p-oe",
        "outputId": "e46a7cad-cd00-4db9-fb51-866cd3283b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 223,635 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 43.31it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 76.83it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.688, train_acc: 0.565\n",
            "valid_loss: 0.658, valid_acc: 0.578\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 43.96it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 76.79it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.372, train_acc: 0.839\n",
            "valid_loss: 0.306, valid_acc: 0.872\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 39.54it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 75.62it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.165, train_acc: 0.941\n",
            "valid_loss: 0.286, valid_acc: 0.885\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 43.92it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 77.48it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.086, train_acc: 0.973\n",
            "valid_loss: 0.325, valid_acc: 0.881\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 43.94it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 71.24it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.061, train_acc: 0.981\n",
            "valid_loss: 0.446, valid_acc: 0.875\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 72.83it/s]\n",
            "test_loss: 0.478, test_acc: 0.872\n"
          ]
        }
      ],
      "source": [
        "adam_width_200_hyperparams = HyperParams()\n",
        "adam_width_200_hyperparams.OPTIM = \"adam\"\n",
        "adam_width_200_hyperparams.LR = 0.001\n",
        "adam_width_200_hyperparams.N_LAYERS = 1\n",
        "#adam_width_200_hyperparams.DROPOUT_RATE = 0.5\n",
        "\n",
        "adam_width_200_hyperparams.HIDDEN_DIM = 200\n",
        "_ = train_and_test_model_with_hparams(adam_width_200_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvVYRGUuuQnF",
        "outputId": "f2371a2d-2588-467c-aa84-f6c5a7ae9c5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 266,485 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 39.55it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 71.11it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.685, train_acc: 0.549\n",
            "valid_loss: 0.633, valid_acc: 0.684\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 40.25it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 69.00it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.492, train_acc: 0.768\n",
            "valid_loss: 0.455, valid_acc: 0.788\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 39.84it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 69.61it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.227, train_acc: 0.911\n",
            "valid_loss: 0.325, valid_acc: 0.873\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 40.05it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 69.87it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.117, train_acc: 0.961\n",
            "valid_loss: 0.324, valid_acc: 0.879\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 39.80it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 69.11it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.060, train_acc: 0.983\n",
            "valid_loss: 0.396, valid_acc: 0.866\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 68.64it/s]\n",
            "test_loss: 0.435, test_acc: 0.859\n"
          ]
        }
      ],
      "source": [
        "adam_width_225_hyperparams = HyperParams()\n",
        "adam_width_225_hyperparams.OPTIM = \"adam\"\n",
        "adam_width_225_hyperparams.LR = 0.001\n",
        "adam_width_225_hyperparams.N_LAYERS = 1\n",
        "#adam_width_225_hyperparams.DROPOUT_RATE = 0.5\n",
        "\n",
        "adam_width_225_hyperparams.HIDDEN_DIM = 225\n",
        "_ = train_and_test_model_with_hparams(adam_width_225_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuI6jhkrqL0O",
        "outputId": "5359c962-6f56-44c9-b01a-f0f4e8c6e94c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 314,335 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 37.73it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 69.38it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.704, train_acc: 0.549\n",
            "valid_loss: 0.658, valid_acc: 0.605\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.17it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 67.00it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.505, train_acc: 0.763\n",
            "valid_loss: 0.378, valid_acc: 0.852\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.26it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 67.95it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.227, train_acc: 0.911\n",
            "valid_loss: 0.307, valid_acc: 0.887\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.21it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 68.30it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.121, train_acc: 0.960\n",
            "valid_loss: 0.324, valid_acc: 0.866\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 35.61it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 68.12it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.071, train_acc: 0.979\n",
            "valid_loss: 0.425, valid_acc: 0.881\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 67.21it/s]\n",
            "test_loss: 0.456, test_acc: 0.870\n"
          ]
        }
      ],
      "source": [
        "adam_width_250_hyperparams = HyperParams()\n",
        "adam_width_250_hyperparams.OPTIM = \"adam\"\n",
        "adam_width_250_hyperparams.LR = 0.001\n",
        "adam_width_250_hyperparams.N_LAYERS = 1\n",
        "#adam_width_250_hyperparams.DROPOUT_RATE = 0.5\n",
        "\n",
        "adam_width_250_hyperparams.HIDDEN_DIM = 250\n",
        "_ = train_and_test_model_with_hparams(adam_width_250_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "oztAB2xFuVk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f1625b-206b-4fb4-f325-81f4f1d80e42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 367,185 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.87it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 67.30it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.687, train_acc: 0.550\n",
            "valid_loss: 0.633, valid_acc: 0.699\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 35.09it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 66.00it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.456, train_acc: 0.802\n",
            "valid_loss: 0.332, valid_acc: 0.865\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 35.16it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 64.73it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.243, train_acc: 0.907\n",
            "valid_loss: 0.354, valid_acc: 0.872\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 35.06it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 66.25it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.134, train_acc: 0.954\n",
            "valid_loss: 0.334, valid_acc: 0.866\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 35.35it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 65.76it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.073, train_acc: 0.979\n",
            "valid_loss: 0.425, valid_acc: 0.876\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 64.41it/s]\n",
            "test_loss: 0.464, test_acc: 0.870\n"
          ]
        }
      ],
      "source": [
        "adam_width_275_hyperparams = HyperParams()\n",
        "adam_width_275_hyperparams.OPTIM = \"adam\"\n",
        "adam_width_275_hyperparams.LR = 0.001\n",
        "adam_width_275_hyperparams.N_LAYERS = 1\n",
        "#adam_width_275_hyperparams.DROPOUT_RATE = 0.5\n",
        "\n",
        "adam_width_275_hyperparams.HIDDEN_DIM = 275\n",
        "_ = train_and_test_model_with_hparams(adam_width_275_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "WEId5d6DKxDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0480f74-8d69-4a8d-b688-ffadbacc5bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 425,035 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 33.42it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 63.94it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.682, train_acc: 0.562\n",
            "valid_loss: 0.592, valid_acc: 0.733\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 33.95it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 63.60it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.372, train_acc: 0.847\n",
            "valid_loss: 0.317, valid_acc: 0.868\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 33.94it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 65.01it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.177, train_acc: 0.936\n",
            "valid_loss: 0.304, valid_acc: 0.883\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 33.36it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 63.04it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.100, train_acc: 0.967\n",
            "valid_loss: 0.415, valid_acc: 0.876\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 33.79it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 63.64it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.061, train_acc: 0.982\n",
            "valid_loss: 0.413, valid_acc: 0.872\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 60.49it/s]\n",
            "test_loss: 0.454, test_acc: 0.862\n"
          ]
        }
      ],
      "source": [
        "adam_width_300_hyperparams = HyperParams()\n",
        "adam_width_300_hyperparams.OPTIM = \"adam\"\n",
        "adam_width_300_hyperparams.LR = 0.001\n",
        "adam_width_300_hyperparams.N_LAYERS = 1\n",
        "#adam_width_300_hyperparams.DROPOUT_RATE = 0.5\n",
        "\n",
        "adam_width_300_hyperparams.HIDDEN_DIM = 300\n",
        "_ = train_and_test_model_with_hparams(adam_width_300_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xOjRj7FsuasW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03e378c5-77a5-4c87-e444-dc9bc87b4498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 474,915 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 33.20it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 63.97it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.686, train_acc: 0.555\n",
            "valid_loss: 0.651, valid_acc: 0.696\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 33.51it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 65.09it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.406, train_acc: 0.819\n",
            "valid_loss: 0.304, valid_acc: 0.882\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 33.47it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 63.98it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.175, train_acc: 0.936\n",
            "valid_loss: 0.337, valid_acc: 0.854\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 29.80it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 61.92it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.095, train_acc: 0.969\n",
            "valid_loss: 0.344, valid_acc: 0.886\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 31.04it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 44.18it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.051, train_acc: 0.985\n",
            "valid_loss: 0.453, valid_acc: 0.879\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 59.00it/s]\n",
            "test_loss: 0.499, test_acc: 0.870\n"
          ]
        }
      ],
      "source": [
        "adam_width_320_hyperparams = HyperParams()\n",
        "adam_width_320_hyperparams.OPTIM = \"adam\"\n",
        "adam_width_320_hyperparams.LR = 0.001\n",
        "adam_width_320_hyperparams.N_LAYERS = 1\n",
        "#adam_width_320_hyperparams.DROPOUT_RATE = 0.5\n",
        "\n",
        "adam_width_320_hyperparams.HIDDEN_DIM = 320\n",
        "_ = train_and_test_model_with_hparams(adam_width_320_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMyCMtqDKxDm"
      },
      "source": [
        "### Lab 2 (e) Larger Embedding Table"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam_embed_1_hyperparams =  HyperParams()\n",
        "adam_embed_1_hyperparams.OPTIM = \"adam\"\n",
        "adam_embed_1_hyperparams.LR = 0.001\n",
        "adam_embed_1_hyperparams.N_LAYERS = 1\n",
        "#adam_embed_1_hyperparams.DROPOUT_RATE = 0.5\n",
        "adam_embed_1_hyperparams.HIDDEN_DIM = 175"
      ],
      "metadata": {
        "id": "OKOn6E-qzaDP"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "GDGkIBv5KxDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bcf1b42-1bdd-4f36-e16e-9649ff4e9384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 247,318 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 35.04it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 80.93it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.660, train_acc: 0.587\n",
            "valid_loss: 0.557, valid_acc: 0.729\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 41.30it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 53.96it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.300, train_acc: 0.878\n",
            "valid_loss: 0.283, valid_acc: 0.891\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 40.51it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 79.58it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.132, train_acc: 0.955\n",
            "valid_loss: 0.312, valid_acc: 0.887\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 43.64it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 45.03it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.063, train_acc: 0.981\n",
            "valid_loss: 0.384, valid_acc: 0.876\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 39.31it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 76.88it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.037, train_acc: 0.990\n",
            "valid_loss: 0.487, valid_acc: 0.871\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 75.61it/s]\n",
            "test_loss: 0.531, test_acc: 0.862\n"
          ]
        }
      ],
      "source": [
        "adam_embed_2_hyperparams = HyperParams()\n",
        "adam_embed_2_hyperparams.OPTIM = \"adam\"\n",
        "adam_embed_2_hyperparams.LR = 0.001\n",
        "adam_embed_2_hyperparams.N_LAYERS = 1\n",
        "#adam_embed_2_hyperparams.DROPOUT_RATE = 0.5\n",
        "adam_embed_2_hyperparams.HIDDEN_DIM = 175\n",
        "\n",
        "adam_embed_2_hyperparams.EMBEDDING_DIM = 2\n",
        "_ = train_and_test_model_with_hparams(adam_embed_2_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam_embed_4_hyperparams = HyperParams()\n",
        "adam_embed_4_hyperparams.OPTIM = \"adam\"\n",
        "adam_embed_4_hyperparams.LR = 0.001\n",
        "adam_embed_4_hyperparams.N_LAYERS = 1\n",
        "#adam_embed_4_hyperparams.DROPOUT_RATE = 0.5\n",
        "adam_embed_4_hyperparams.HIDDEN_DIM = 175\n",
        "\n",
        "adam_embed_4_hyperparams.EMBEDDING_DIM = 4\n",
        "_ = train_and_test_model_with_hparams(adam_embed_4_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18Jk-eXhnp3s",
        "outputId": "f34d7ddc-eb58-45ac-a379-68fb436c165a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 370,384 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 36.92it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 73.66it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.614, train_acc: 0.629\n",
            "valid_loss: 0.360, valid_acc: 0.846\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 35.47it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 56.08it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.268, train_acc: 0.895\n",
            "valid_loss: 0.315, valid_acc: 0.879\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.29it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 77.80it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.128, train_acc: 0.957\n",
            "valid_loss: 0.339, valid_acc: 0.862\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 40.82it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 77.85it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.068, train_acc: 0.979\n",
            "valid_loss: 0.423, valid_acc: 0.869\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 45.82it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 77.68it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.036, train_acc: 0.990\n",
            "valid_loss: 0.639, valid_acc: 0.864\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 77.41it/s]\n",
            "test_loss: 0.672, test_acc: 0.863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam_embed_8_hyperparams = HyperParams()\n",
        "adam_embed_8_hyperparams.OPTIM = \"adam\"\n",
        "adam_embed_8_hyperparams.LR = 0.001\n",
        "adam_embed_8_hyperparams.N_LAYERS = 1\n",
        "#adam_embed_8_hyperparams.DROPOUT_RATE = 0.5\n",
        "adam_embed_8_hyperparams.HIDDEN_DIM = 175\n",
        "\n",
        "adam_embed_8_hyperparams.EMBEDDING_DIM = 8\n",
        "_ = train_and_test_model_with_hparams(adam_embed_8_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbycminjnqOO",
        "outputId": "cd40814c-655c-4485-db7f-bf433fe2af9b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 616,516 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 42.41it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 77.66it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.573, train_acc: 0.667\n",
            "valid_loss: 0.322, valid_acc: 0.867\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 40.84it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 79.10it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.223, train_acc: 0.914\n",
            "valid_loss: 0.309, valid_acc: 0.873\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 45.42it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 80.30it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.099, train_acc: 0.968\n",
            "valid_loss: 0.375, valid_acc: 0.869\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.30it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 77.42it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.049, train_acc: 0.986\n",
            "valid_loss: 0.469, valid_acc: 0.860\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 43.41it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 53.04it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.058, train_acc: 0.980\n",
            "valid_loss: 0.547, valid_acc: 0.858\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 58.44it/s]\n",
            "test_loss: 0.566, test_acc: 0.853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam_embed_16_hyperparams = HyperParams()\n",
        "adam_embed_16_hyperparams.OPTIM = \"adam\"\n",
        "adam_embed_16_hyperparams.LR = 0.001\n",
        "adam_embed_16_hyperparams.N_LAYERS = 1\n",
        "#adam_embed_16_hyperparams.DROPOUT_RATE = 0.5\n",
        "adam_embed_16_hyperparams.HIDDEN_DIM = 175\n",
        "\n",
        "adam_embed_16_hyperparams.EMBEDDING_DIM = 16\n",
        "_ = train_and_test_model_with_hparams(adam_embed_16_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQZU_9V6nsIH",
        "outputId": "9664e0f6-846c-405d-b57a-753e051a7f1f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 1,108,780 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 43.46it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 78.45it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.487, train_acc: 0.729\n",
            "valid_loss: 0.507, valid_acc: 0.742\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 43.41it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 78.74it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.208, train_acc: 0.923\n",
            "valid_loss: 0.350, valid_acc: 0.851\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.61it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 77.48it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.101, train_acc: 0.968\n",
            "valid_loss: 0.395, valid_acc: 0.873\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.35it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 77.94it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.056, train_acc: 0.982\n",
            "valid_loss: 0.477, valid_acc: 0.853\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.86it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 75.68it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.035, train_acc: 0.989\n",
            "valid_loss: 0.591, valid_acc: 0.861\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 74.35it/s]\n",
            "test_loss: 0.609, test_acc: 0.853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam_embed_32_hyperparams = HyperParams()\n",
        "adam_embed_32_hyperparams.OPTIM = \"adam\"\n",
        "adam_embed_32_hyperparams.LR = 0.001\n",
        "adam_embed_32_hyperparams.N_LAYERS = 1\n",
        "#adam_embed_32_hyperparams.DROPOUT_RATE = 0.5\n",
        "adam_embed_32_hyperparams.HIDDEN_DIM = 175\n",
        "\n",
        "adam_embed_32_hyperparams.EMBEDDING_DIM = 32\n",
        "_ = train_and_test_model_with_hparams(adam_embed_32_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sszgCgZjnsRP",
        "outputId": "a9bb4184-d659-47a3-d61e-47f41729d79a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 2,093,308 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 39.91it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 76.31it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.513, train_acc: 0.711\n",
            "valid_loss: 0.305, valid_acc: 0.872\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.01it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 77.51it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.206, train_acc: 0.923\n",
            "valid_loss: 0.329, valid_acc: 0.872\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.65it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 77.76it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.093, train_acc: 0.970\n",
            "valid_loss: 0.376, valid_acc: 0.863\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.33it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 71.13it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.045, train_acc: 0.986\n",
            "valid_loss: 0.563, valid_acc: 0.868\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 39.97it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 79.24it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.029, train_acc: 0.990\n",
            "valid_loss: 0.605, valid_acc: 0.863\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 75.76it/s]\n",
            "test_loss: 0.663, test_acc: 0.856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam_embed_64_hyperparams = HyperParams()\n",
        "adam_embed_64_hyperparams.OPTIM = \"adam\"\n",
        "adam_embed_64_hyperparams.LR = 0.001\n",
        "adam_embed_64_hyperparams.N_LAYERS = 1\n",
        "#adam_embed_64_hyperparams.DROPOUT_RATE = 0.5\n",
        "adam_embed_64_hyperparams.HIDDEN_DIM = 175\n",
        "\n",
        "adam_embed_64_hyperparams.EMBEDDING_DIM = 64\n",
        "_ = train_and_test_model_with_hparams(adam_embed_64_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHMwRGG3nsr1",
        "outputId": "ad8530ed-76da-423d-e728-5db30ac4d894"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 4,062,364 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 41.64it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 76.15it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.454, train_acc: 0.767\n",
            "valid_loss: 0.346, valid_acc: 0.855\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 41.82it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 76.67it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.196, train_acc: 0.930\n",
            "valid_loss: 0.338, valid_acc: 0.870\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 42.15it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 76.50it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.103, train_acc: 0.965\n",
            "valid_loss: 0.451, valid_acc: 0.849\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 42.19it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 72.42it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.054, train_acc: 0.982\n",
            "valid_loss: 0.532, valid_acc: 0.853\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 41.60it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 74.90it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.039, train_acc: 0.987\n",
            "valid_loss: 0.507, valid_acc: 0.839\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 72.89it/s]\n",
            "test_loss: 0.560, test_acc: 0.834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam_embed_128_hyperparams = HyperParams()\n",
        "adam_embed_128_hyperparams.OPTIM = \"adam\"\n",
        "adam_embed_128_hyperparams.LR = 0.001\n",
        "adam_embed_128_hyperparams.N_LAYERS = 1\n",
        "#adam_embed_128_hyperparams.DROPOUT_RATE = 0.5\n",
        "adam_embed_128_hyperparams.HIDDEN_DIM = 175\n",
        "\n",
        "adam_embed_128_hyperparams.EMBEDDING_DIM = 128\n",
        "_ = train_and_test_model_with_hparams(adam_embed_128_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxZbnDGInsvn",
        "outputId": "6eba5a83-f8e4-4f3b-f6fa-79c2dbbbefd1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 8,000,476 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 37.74it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 75.98it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.472, train_acc: 0.752\n",
            "valid_loss: 0.320, valid_acc: 0.868\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.18it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 76.13it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.201, train_acc: 0.926\n",
            "valid_loss: 0.334, valid_acc: 0.864\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.40it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 77.06it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.088, train_acc: 0.970\n",
            "valid_loss: 0.365, valid_acc: 0.864\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.10it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 77.18it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.055, train_acc: 0.981\n",
            "valid_loss: 0.522, valid_acc: 0.869\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.13it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 77.30it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.025, train_acc: 0.992\n",
            "valid_loss: 0.637, valid_acc: 0.836\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 73.94it/s]\n",
            "test_loss: 0.667, test_acc: 0.830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam_embed_256_hyperparams = HyperParams()\n",
        "adam_embed_256_hyperparams.OPTIM = \"adam\"\n",
        "adam_embed_256_hyperparams.LR = 0.001\n",
        "adam_embed_256_hyperparams.N_LAYERS = 1\n",
        "#adam_embed_256_hyperparams.DROPOUT_RATE = 0.5\n",
        "adam_embed_256_hyperparams.HIDDEN_DIM = 175\n",
        "\n",
        "adam_embed_256_hyperparams.EMBEDDING_DIM = 256\n",
        "_ = train_and_test_model_with_hparams(adam_embed_256_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njHVInX7nzH5",
        "outputId": "c920e2b0-6691-417f-e834-62cf4c06fbb5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 15,876,700 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 30.27it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 69.80it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.444, train_acc: 0.774\n",
            "valid_loss: 0.304, valid_acc: 0.875\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 31.30it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 73.56it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.200, train_acc: 0.927\n",
            "valid_loss: 0.411, valid_acc: 0.845\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 31.76it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 73.74it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.093, train_acc: 0.969\n",
            "valid_loss: 0.377, valid_acc: 0.874\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 31.54it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 75.11it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.053, train_acc: 0.984\n",
            "valid_loss: 0.484, valid_acc: 0.853\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 31.78it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 74.72it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.038, train_acc: 0.987\n",
            "valid_loss: 0.553, valid_acc: 0.857\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 73.43it/s]\n",
            "test_loss: 0.603, test_acc: 0.848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJiOAkGNKxDn"
      },
      "source": [
        "### Lab 2(f) Compound scaling of embedding_dim, hidden_dim, layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EfficientNet:\n",
        "\n",
        "```\n",
        "params_dict = {\n",
        "      # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n",
        "      'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n",
        "      'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n",
        "      'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n",
        "      'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n",
        "      'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n",
        "      'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n",
        "      'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n",
        "      'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n",
        "}\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9dnrL3k91VYC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "WztYxt1IKxDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "788948c1-df77-4ac8-a044-7312a24887d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N_LAYERS: 1\n",
            "HIDDEN_DIM: 28\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 64,363 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.26it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.90it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.596, train_acc: 0.634\n",
            "valid_loss: 0.339, valid_acc: 0.858\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 52.11it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 90.38it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.232, train_acc: 0.911\n",
            "valid_loss: 0.278, valid_acc: 0.887\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 52.64it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 89.15it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.113, train_acc: 0.964\n",
            "valid_loss: 0.289, valid_acc: 0.891\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.17it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 90.22it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.055, train_acc: 0.984\n",
            "valid_loss: 0.341, valid_acc: 0.887\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 52.42it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 90.00it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.028, train_acc: 0.992\n",
            "valid_loss: 0.496, valid_acc: 0.876\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 84.58it/s]\n",
            "test_loss: 0.539, test_acc: 0.874\n",
            "\n",
            "N_LAYERS: 2\n",
            "HIDDEN_DIM: 30\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 72,295 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 35.73it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 65.99it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.677, train_acc: 0.575\n",
            "valid_loss: 0.521, valid_acc: 0.719\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.15it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 74.34it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.330, train_acc: 0.864\n",
            "valid_loss: 0.320, valid_acc: 0.861\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.43it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 74.24it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.159, train_acc: 0.944\n",
            "valid_loss: 0.318, valid_acc: 0.869\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 37.94it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 74.33it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.080, train_acc: 0.976\n",
            "valid_loss: 0.369, valid_acc: 0.881\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 36.33it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 70.18it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.043, train_acc: 0.989\n",
            "valid_loss: 0.422, valid_acc: 0.871\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 68.07it/s]\n",
            "test_loss: 0.456, test_acc: 0.861\n",
            "\n",
            "N_LAYERS: 3\n",
            "HIDDEN_DIM: 33\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 83,605 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 30.18it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 60.61it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.579, train_acc: 0.630\n",
            "valid_loss: 0.346, valid_acc: 0.857\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 29.67it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 59.48it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.240, train_acc: 0.907\n",
            "valid_loss: 0.263, valid_acc: 0.896\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 30.04it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 60.72it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.111, train_acc: 0.964\n",
            "valid_loss: 0.345, valid_acc: 0.886\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 29.83it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 60.53it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.047, train_acc: 0.986\n",
            "valid_loss: 0.477, valid_acc: 0.875\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 30.12it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 60.19it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.026, train_acc: 0.992\n",
            "valid_loss: 0.521, valid_acc: 0.876\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 58.62it/s]\n",
            "test_loss: 0.566, test_acc: 0.872\n",
            "\n",
            "N_LAYERS: 4\n",
            "HIDDEN_DIM: 37\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 100,573 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:15<00:00, 23.48it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 52.78it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.498\n",
            "valid_loss: 0.693, valid_acc: 0.496\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 24.89it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 53.53it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.552, train_acc: 0.664\n",
            "valid_loss: 0.315, valid_acc: 0.869\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 24.85it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 53.29it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.240, train_acc: 0.909\n",
            "valid_loss: 0.338, valid_acc: 0.865\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 24.65it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 52.37it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.120, train_acc: 0.960\n",
            "valid_loss: 0.291, valid_acc: 0.885\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 24.77it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 53.16it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.060, train_acc: 0.983\n",
            "valid_loss: 0.404, valid_acc: 0.877\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:02<00:00, 52.28it/s]\n",
            "test_loss: 0.430, test_acc: 0.871\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Perform gri\n",
        "compounding_hyperparams = adam_optimizer_hyperparams\n",
        "compounding_hyperparams.OPTIM = \"adam\"\n",
        "compounding_hyperparams.LR = 0.001\n",
        "compounding_hyperparams.N_LAYERS = 1\n",
        "#adam_embed_256_hyperparams.DROPOUT_RATE = 0.5\n",
        "compounding_hyperparams.HIDDEN_DIM = 25\n",
        "compounding_hyperparams.EMBEDDING_DIM = 1\n",
        "\n",
        "#Numbers used in EfficientNet paper\n",
        "#depth coefficient * width coefficient^2 * embed_coefficient ^2 = 2\n",
        "depth_coefficient = 1.4\n",
        "width_coefficient = 1.1\n",
        "resolution_coefficient = 1.1\n",
        "\n",
        "current_n_layers= compounding_hyperparams.N_LAYERS\n",
        "current_hidden_dim = compounding_hyperparams.HIDDEN_DIM\n",
        "current_embedding_dim = compounding_hyperparams.EMBEDDING_DIM\n",
        "\n",
        "for i in range(1,5):\n",
        "  current_n_layers = current_n_layers * depth_coefficient\n",
        "  current_hidden_dim = current_hidden_dim * width_coefficient\n",
        "  current_embedding_dim = current_embedding_dim * resolution_coefficient\n",
        "  compounding_hyperparams.N_LAYERS = round(current_n_layers)\n",
        "  compounding_hyperparams.HIDDEN_DIM = round(current_hidden_dim)\n",
        "  compounding_hyperparams.EMBEDDING_DIM = round(current_embedding_dim)\n",
        "  print(f'N_LAYERS: {compounding_hyperparams.N_LAYERS }')\n",
        "  print(f'HIDDEN_DIM: {compounding_hyperparams.HIDDEN_DIM }')\n",
        "  print(f'EMBEDDING_DIM: {compounding_hyperparams.EMBEDDING_DIM }')\n",
        "  _ = train_and_test_model_with_hparams(compounding_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform gri\n",
        "compounding_hyperparams = HyperParams()\n",
        "compounding_hyperparams.OPTIM = \"adam\"\n",
        "compounding_hyperparams.LR = 0.001\n",
        "compounding_hyperparams.N_LAYERS = 1\n",
        "#adam_embed_256_hyperparams.DROPOUT_RATE = 0.5\n",
        "compounding_hyperparams.HIDDEN_DIM = 100\n",
        "compounding_hyperparams.EMBEDDING_DIM = 1\n",
        "\n",
        "#Numbers used in EfficientNet paper\n",
        "#depth coefficient * width coefficient^2 * embed_coefficient ^2 = 2\n",
        "depth_coefficient = 1.2\n",
        "width_coefficient = 1.1\n",
        "resolution_coefficient = 1.15\n",
        "\n",
        "current_n_layers= compounding_hyperparams.N_LAYERS\n",
        "current_hidden_dim = compounding_hyperparams.HIDDEN_DIM\n",
        "current_embedding_dim = compounding_hyperparams.EMBEDDING_DIM\n",
        "\n",
        "for i in range(1,5):\n",
        "  current_n_layers = current_n_layers * depth_coefficient\n",
        "  current_hidden_dim = current_hidden_dim * width_coefficient\n",
        "  current_embedding_dim = current_embedding_dim * resolution_coefficient\n",
        "  compounding_hyperparams.N_LAYERS = round(current_n_layers)\n",
        "  compounding_hyperparams.HIDDEN_DIM = round(current_hidden_dim)\n",
        "  compounding_hyperparams.EMBEDDING_DIM = round(current_embedding_dim)\n",
        "  print(f'N_LAYERS: {compounding_hyperparams.N_LAYERS }')\n",
        "  print(f'HIDDEN_DIM: {compounding_hyperparams.HIDDEN_DIM }')\n",
        "  print(f'EMBEDDING_DIM: {compounding_hyperparams.EMBEDDING_DIM }')\n",
        "  _ = train_and_test_model_with_hparams(compounding_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIpXYDU-2U5i",
        "outputId": "30ac9533-fc43-4e5e-8e28-a6d0d0be2c9a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N_LAYERS: 1\n",
            "HIDDEN_DIM: 110\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 110,775 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 45.80it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.72it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.642, train_acc: 0.603\n",
            "valid_loss: 0.337, valid_acc: 0.858\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 47.16it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 71.44it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.257, train_acc: 0.899\n",
            "valid_loss: 0.268, valid_acc: 0.889\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 32.52it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 82.43it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.136, train_acc: 0.955\n",
            "valid_loss: 0.287, valid_acc: 0.886\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 47.62it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 80.31it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.076, train_acc: 0.977\n",
            "valid_loss: 0.364, valid_acc: 0.884\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 47.82it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.42it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.050, train_acc: 0.986\n",
            "valid_loss: 0.454, valid_acc: 0.878\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 82.88it/s]\n",
            "test_loss: 0.504, test_acc: 0.872\n",
            "\n",
            "N_LAYERS: 1\n",
            "HIDDEN_DIM: 121\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 121,093 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 46.08it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 83.24it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.613, train_acc: 0.615\n",
            "valid_loss: 0.383, valid_acc: 0.845\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.54it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 82.68it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.251, train_acc: 0.902\n",
            "valid_loss: 0.276, valid_acc: 0.890\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 46.15it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.55it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.134, train_acc: 0.954\n",
            "valid_loss: 0.314, valid_acc: 0.888\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 46.45it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 56.82it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.069, train_acc: 0.979\n",
            "valid_loss: 0.370, valid_acc: 0.878\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 40.18it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 81.56it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.035, train_acc: 0.991\n",
            "valid_loss: 0.498, valid_acc: 0.873\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 76.91it/s]\n",
            "test_loss: 0.534, test_acc: 0.872\n",
            "\n",
            "N_LAYERS: 2\n",
            "HIDDEN_DIM: 133\n",
            "EMBEDDING_DIM: 2\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 337,394 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 30.80it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 67.03it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.591, train_acc: 0.636\n",
            "valid_loss: 0.349, valid_acc: 0.849\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 31.75it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 67.71it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.231, train_acc: 0.911\n",
            "valid_loss: 0.292, valid_acc: 0.882\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 31.51it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 65.07it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.099, train_acc: 0.968\n",
            "valid_loss: 0.379, valid_acc: 0.862\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 31.05it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 64.43it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.050, train_acc: 0.986\n",
            "valid_loss: 0.457, valid_acc: 0.867\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 29.55it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 66.90it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.032, train_acc: 0.990\n",
            "valid_loss: 0.511, valid_acc: 0.847\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 67.38it/s]\n",
            "test_loss: 0.550, test_acc: 0.838\n",
            "\n",
            "N_LAYERS: 2\n",
            "HIDDEN_DIM: 146\n",
            "EMBEDDING_DIM: 2\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 381,256 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 29.88it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 67.25it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.632, train_acc: 0.616\n",
            "valid_loss: 0.637, valid_acc: 0.742\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 30.93it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 66.10it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.305, train_acc: 0.874\n",
            "valid_loss: 0.311, valid_acc: 0.872\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 30.90it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 65.00it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.149, train_acc: 0.948\n",
            "valid_loss: 0.343, valid_acc: 0.868\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 31.09it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 65.96it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.068, train_acc: 0.979\n",
            "valid_loss: 0.495, valid_acc: 0.861\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 30.53it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 65.53it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.033, train_acc: 0.991\n",
            "valid_loss: 0.561, valid_acc: 0.855\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 64.91it/s]\n",
            "test_loss: 0.588, test_acc: 0.853\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform gri\n",
        "compounding_hyperparams = HyperParams()\n",
        "compounding_hyperparams.OPTIM = \"adam\"\n",
        "compounding_hyperparams.LR = 0.001\n",
        "compounding_hyperparams.N_LAYERS = 1\n",
        "#adam_embed_256_hyperparams.DROPOUT_RATE = 0.5\n",
        "compounding_hyperparams.HIDDEN_DIM = 100\n",
        "compounding_hyperparams.EMBEDDING_DIM = 1\n",
        "\n",
        "#Numbers used in EfficientNet paper\n",
        "#depth coefficient * width coefficient^2 * embed_coefficient ^2 = 2\n",
        "depth_coefficient = 1.4\n",
        "width_coefficient = 1.1\n",
        "resolution_coefficient = 1.1\n",
        "\n",
        "current_n_layers= compounding_hyperparams.N_LAYERS\n",
        "current_hidden_dim = compounding_hyperparams.HIDDEN_DIM\n",
        "current_embedding_dim = compounding_hyperparams.EMBEDDING_DIM\n",
        "\n",
        "for i in range(1,5):\n",
        "  current_n_layers = current_n_layers * depth_coefficient\n",
        "  current_hidden_dim = current_hidden_dim * width_coefficient\n",
        "  current_embedding_dim = current_embedding_dim * resolution_coefficient\n",
        "  compounding_hyperparams.N_LAYERS = round(current_n_layers)\n",
        "  compounding_hyperparams.HIDDEN_DIM = round(current_hidden_dim)\n",
        "  compounding_hyperparams.EMBEDDING_DIM = round(current_embedding_dim)\n",
        "  print(f'N_LAYERS: {compounding_hyperparams.N_LAYERS }')\n",
        "  print(f'HIDDEN_DIM: {compounding_hyperparams.HIDDEN_DIM }')\n",
        "  print(f'EMBEDDING_DIM: {compounding_hyperparams.EMBEDDING_DIM }')\n",
        "  _ = train_and_test_model_with_hparams(compounding_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKcza4UC4Ftk",
        "outputId": "4fd50421-7bea-4813-be55-442e47cca4b4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N_LAYERS: 1\n",
            "HIDDEN_DIM: 110\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 110,775 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 42.67it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.06it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.642, train_acc: 0.603\n",
            "valid_loss: 0.337, valid_acc: 0.858\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 43.22it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 38.94it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.257, train_acc: 0.899\n",
            "valid_loss: 0.268, valid_acc: 0.889\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.63it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 83.74it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.136, train_acc: 0.955\n",
            "valid_loss: 0.287, valid_acc: 0.886\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 46.81it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.35it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.076, train_acc: 0.977\n",
            "valid_loss: 0.364, valid_acc: 0.884\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 46.58it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.19it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.050, train_acc: 0.986\n",
            "valid_loss: 0.454, valid_acc: 0.878\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 80.50it/s]\n",
            "test_loss: 0.504, test_acc: 0.872\n",
            "\n",
            "N_LAYERS: 2\n",
            "HIDDEN_DIM: 121\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 239,189 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 30.93it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 68.72it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.617, train_acc: 0.629\n",
            "valid_loss: 0.493, valid_acc: 0.786\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 32.30it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 63.26it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.304, train_acc: 0.880\n",
            "valid_loss: 0.344, valid_acc: 0.855\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 30.89it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 65.61it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.148, train_acc: 0.948\n",
            "valid_loss: 0.320, valid_acc: 0.883\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 32.12it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 67.38it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.070, train_acc: 0.979\n",
            "valid_loss: 0.460, valid_acc: 0.874\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 31.74it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 66.15it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.042, train_acc: 0.986\n",
            "valid_loss: 0.529, valid_acc: 0.870\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 66.45it/s]\n",
            "test_loss: 0.570, test_acc: 0.864\n",
            "\n",
            "N_LAYERS: 3\n",
            "HIDDEN_DIM: 133\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 418,605 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 24.37it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 56.72it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.641, train_acc: 0.586\n",
            "valid_loss: 0.475, valid_acc: 0.801\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:17<00:00, 20.98it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 53.39it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.354, train_acc: 0.853\n",
            "valid_loss: 0.355, valid_acc: 0.847\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:16<00:00, 21.61it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 56.19it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.179, train_acc: 0.934\n",
            "valid_loss: 0.294, valid_acc: 0.885\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 24.49it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 56.16it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.099, train_acc: 0.968\n",
            "valid_loss: 0.371, valid_acc: 0.866\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:15<00:00, 23.88it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 56.35it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.054, train_acc: 0.984\n",
            "valid_loss: 0.440, valid_acc: 0.869\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 56.50it/s]\n",
            "test_loss: 0.475, test_acc: 0.860\n",
            "\n",
            "N_LAYERS: 4\n",
            "HIDDEN_DIM: 146\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 663,231 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:19<00:00, 19.12it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 46.63it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.499\n",
            "valid_loss: 0.693, valid_acc: 0.504\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:22<00:00, 16.39it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 48.19it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.693, train_acc: 0.499\n",
            "valid_loss: 0.690, valid_acc: 0.506\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:19<00:00, 18.30it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 45.53it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.542, train_acc: 0.724\n",
            "valid_loss: 0.470, valid_acc: 0.791\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:20<00:00, 17.49it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 39.55it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.271, train_acc: 0.892\n",
            "valid_loss: 0.298, valid_acc: 0.876\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:21<00:00, 16.69it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 47.20it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.147, train_acc: 0.948\n",
            "valid_loss: 0.328, valid_acc: 0.877\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:02<00:00, 46.46it/s]\n",
            "test_loss: 0.352, test_acc: 0.868\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform gri\n",
        "compounding_hyperparams = HyperParams()\n",
        "compounding_hyperparams.OPTIM = \"adam\"\n",
        "compounding_hyperparams.LR = 0.001\n",
        "compounding_hyperparams.N_LAYERS = 1\n",
        "#adam_embed_256_hyperparams.DROPOUT_RATE = 0.5\n",
        "compounding_hyperparams.HIDDEN_DIM = 150\n",
        "compounding_hyperparams.EMBEDDING_DIM = 1\n",
        "\n",
        "#Numbers used in EfficientNet paper\n",
        "#depth coefficient * width coefficient^2 * embed_coefficient ^2 = 2\n",
        "depth_coefficient = 1.4\n",
        "width_coefficient = 1.1\n",
        "resolution_coefficient = 1.1\n",
        "\n",
        "current_n_layers= compounding_hyperparams.N_LAYERS\n",
        "current_hidden_dim = compounding_hyperparams.HIDDEN_DIM\n",
        "current_embedding_dim = compounding_hyperparams.EMBEDDING_DIM\n",
        "\n",
        "for i in range(1,5):\n",
        "  current_n_layers = current_n_layers * depth_coefficient\n",
        "  current_hidden_dim = current_hidden_dim * width_coefficient\n",
        "  current_embedding_dim = current_embedding_dim * resolution_coefficient\n",
        "  compounding_hyperparams.N_LAYERS = round(current_n_layers)\n",
        "  compounding_hyperparams.HIDDEN_DIM = round(current_hidden_dim)\n",
        "  compounding_hyperparams.EMBEDDING_DIM = round(current_embedding_dim)\n",
        "  print(f'N_LAYERS: {compounding_hyperparams.N_LAYERS }')\n",
        "  print(f'HIDDEN_DIM: {compounding_hyperparams.HIDDEN_DIM }')\n",
        "  print(f'EMBEDDING_DIM: {compounding_hyperparams.EMBEDDING_DIM }')\n",
        "  _ = train_and_test_model_with_hparams(compounding_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob46RrO74PAM",
        "outputId": "3da8ac27-b354-4c75-941a-df79ac0cdcb8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N_LAYERS: 1\n",
            "HIDDEN_DIM: 165\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 172,045 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.21it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 77.53it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.643, train_acc: 0.597\n",
            "valid_loss: 0.363, valid_acc: 0.839\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 45.59it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 79.34it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.283, train_acc: 0.888\n",
            "valid_loss: 0.287, valid_acc: 0.883\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.50it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 75.84it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.145, train_acc: 0.949\n",
            "valid_loss: 0.320, valid_acc: 0.889\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 42.06it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 75.00it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.085, train_acc: 0.972\n",
            "valid_loss: 0.395, valid_acc: 0.876\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.14it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 79.22it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.047, train_acc: 0.986\n",
            "valid_loss: 0.448, valid_acc: 0.876\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 75.79it/s]\n",
            "test_loss: 0.487, test_acc: 0.863\n",
            "\n",
            "N_LAYERS: 2\n",
            "HIDDEN_DIM: 182\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 462,327 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 28.54it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 63.55it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.637, train_acc: 0.595\n",
            "valid_loss: 0.365, valid_acc: 0.843\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 29.31it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 61.42it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.267, train_acc: 0.893\n",
            "valid_loss: 0.321, valid_acc: 0.881\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 28.96it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 63.77it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.134, train_acc: 0.954\n",
            "valid_loss: 0.326, valid_acc: 0.872\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:13<00:00, 27.32it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 64.32it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.072, train_acc: 0.977\n",
            "valid_loss: 0.424, valid_acc: 0.873\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 28.98it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 61.49it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.044, train_acc: 0.987\n",
            "valid_loss: 0.473, valid_acc: 0.875\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 61.73it/s]\n",
            "test_loss: 0.532, test_acc: 0.860\n",
            "\n",
            "N_LAYERS: 3\n",
            "HIDDEN_DIM: 200\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 866,835 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:17<00:00, 20.61it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 50.60it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.697, train_acc: 0.507\n",
            "valid_loss: 0.692, valid_acc: 0.497\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:17<00:00, 20.52it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 50.00it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.502, train_acc: 0.751\n",
            "valid_loss: 0.370, valid_acc: 0.847\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:17<00:00, 20.52it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 50.04it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.248, train_acc: 0.902\n",
            "valid_loss: 0.308, valid_acc: 0.870\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:17<00:00, 20.50it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 51.54it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.198, train_acc: 0.924\n",
            "valid_loss: 0.404, valid_acc: 0.857\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:17<00:00, 20.62it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 48.10it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.133, train_acc: 0.953\n",
            "valid_loss: 0.366, valid_acc: 0.868\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:02<00:00, 50.80it/s]\n",
            "test_loss: 0.398, test_acc: 0.857\n",
            "\n",
            "N_LAYERS: 4\n",
            "HIDDEN_DIM: 220\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 1,424,395 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:26<00:00, 13.99it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 38.19it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.498\n",
            "valid_loss: 0.693, valid_acc: 0.496\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:25<00:00, 14.26it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 38.47it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.692, train_acc: 0.501\n",
            "valid_loss: 0.687, valid_acc: 0.507\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:26<00:00, 13.99it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 36.65it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.519, train_acc: 0.736\n",
            "valid_loss: 0.598, valid_acc: 0.661\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:29<00:00, 12.33it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 37.69it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.272, train_acc: 0.894\n",
            "valid_loss: 0.330, valid_acc: 0.864\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:25<00:00, 14.14it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 37.55it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.174, train_acc: 0.937\n",
            "valid_loss: 0.327, valid_acc: 0.870\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:02<00:00, 37.06it/s]\n",
            "test_loss: 0.339, test_acc: 0.864\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform gri\n",
        "compounding_hyperparams = HyperParams()\n",
        "compounding_hyperparams.OPTIM = \"adam\"\n",
        "compounding_hyperparams.LR = 0.001\n",
        "compounding_hyperparams.N_LAYERS = 1\n",
        "#adam_embed_256_hyperparams.DROPOUT_RATE = 0.5\n",
        "compounding_hyperparams.HIDDEN_DIM = 20\n",
        "compounding_hyperparams.EMBEDDING_DIM = 1\n",
        "\n",
        "#Numbers used in EfficientNet paper\n",
        "#depth coefficient * width coefficient^2 * embed_coefficient ^2 = 2\n",
        "depth_coefficient = 1.2\n",
        "width_coefficient = 1.3\n",
        "resolution_coefficient = 1\n",
        "\n",
        "current_n_layers= compounding_hyperparams.N_LAYERS\n",
        "current_hidden_dim = compounding_hyperparams.HIDDEN_DIM\n",
        "current_embedding_dim = compounding_hyperparams.EMBEDDING_DIM\n",
        "\n",
        "for i in range(1,5):\n",
        "  current_n_layers = current_n_layers * depth_coefficient\n",
        "  current_hidden_dim = current_hidden_dim * width_coefficient\n",
        "  current_embedding_dim = current_embedding_dim * resolution_coefficient\n",
        "  compounding_hyperparams.N_LAYERS = round(current_n_layers)\n",
        "  compounding_hyperparams.HIDDEN_DIM = round(current_hidden_dim)\n",
        "  compounding_hyperparams.EMBEDDING_DIM = round(current_embedding_dim)\n",
        "  print(f'N_LAYERS: {compounding_hyperparams.N_LAYERS }')\n",
        "  print(f'HIDDEN_DIM: {compounding_hyperparams.HIDDEN_DIM }')\n",
        "  print(f'EMBEDDING_DIM: {compounding_hyperparams.EMBEDDING_DIM }')\n",
        "  _ = train_and_test_model_with_hparams(compounding_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMQxh5ZZ8MAx",
        "outputId": "4b288106-09cb-4d82-ddcc-3a36264faebf"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N_LAYERS: 1\n",
            "HIDDEN_DIM: 26\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 63,903 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 45.06it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.08it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.590, train_acc: 0.634\n",
            "valid_loss: 0.343, valid_acc: 0.859\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 49.97it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 89.20it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.234, train_acc: 0.909\n",
            "valid_loss: 0.307, valid_acc: 0.877\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 49.16it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.58it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.119, train_acc: 0.961\n",
            "valid_loss: 0.274, valid_acc: 0.895\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.95it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.45it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.061, train_acc: 0.983\n",
            "valid_loss: 0.333, valid_acc: 0.883\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.51it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.33it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.031, train_acc: 0.991\n",
            "valid_loss: 0.437, valid_acc: 0.881\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 77.49it/s]\n",
            "test_loss: 0.478, test_acc: 0.875\n",
            "\n",
            "N_LAYERS: 1\n",
            "HIDDEN_DIM: 34\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 65,935 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.42it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 90.55it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.598, train_acc: 0.623\n",
            "valid_loss: 0.338, valid_acc: 0.866\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.48it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.85it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.243, train_acc: 0.906\n",
            "valid_loss: 0.265, valid_acc: 0.891\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.70it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.85it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.120, train_acc: 0.961\n",
            "valid_loss: 0.297, valid_acc: 0.892\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.73it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.18it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.058, train_acc: 0.984\n",
            "valid_loss: 0.386, valid_acc: 0.885\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.53it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 79.87it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.033, train_acc: 0.992\n",
            "valid_loss: 0.468, valid_acc: 0.882\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 82.32it/s]\n",
            "test_loss: 0.521, test_acc: 0.872\n",
            "\n",
            "N_LAYERS: 2\n",
            "HIDDEN_DIM: 44\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 85,035 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 36.13it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 70.68it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.545, train_acc: 0.663\n",
            "valid_loss: 0.293, valid_acc: 0.882\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 36.83it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 71.66it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.216, train_acc: 0.918\n",
            "valid_loss: 0.257, valid_acc: 0.896\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 36.28it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 66.85it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.102, train_acc: 0.967\n",
            "valid_loss: 0.332, valid_acc: 0.881\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 36.67it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 71.58it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.051, train_acc: 0.984\n",
            "valid_loss: 0.400, valid_acc: 0.881\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 36.21it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 71.14it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.025, train_acc: 0.993\n",
            "valid_loss: 0.636, valid_acc: 0.867\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 68.75it/s]\n",
            "test_loss: 0.679, test_acc: 0.865\n",
            "\n",
            "N_LAYERS: 2\n",
            "HIDDEN_DIM: 57\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 101,077 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 35.45it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 72.91it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.589, train_acc: 0.631\n",
            "valid_loss: 0.443, valid_acc: 0.795\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 35.84it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 70.25it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.239, train_acc: 0.907\n",
            "valid_loss: 0.291, valid_acc: 0.877\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 35.57it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 71.21it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.112, train_acc: 0.962\n",
            "valid_loss: 0.310, valid_acc: 0.878\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 36.21it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 72.65it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.051, train_acc: 0.985\n",
            "valid_loss: 0.409, valid_acc: 0.873\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 36.01it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 72.51it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.027, train_acc: 0.992\n",
            "valid_loss: 0.531, valid_acc: 0.860\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 68.80it/s]\n",
            "test_loss: 0.576, test_acc: 0.848\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform gri\n",
        "compounding_hyperparams = HyperParams()\n",
        "compounding_hyperparams.OPTIM = \"adam\"\n",
        "compounding_hyperparams.LR = 0.001\n",
        "compounding_hyperparams.N_LAYERS = 1\n",
        "compounding_hyperparams.DROPOUT_RATE = 0.5\n",
        "compounding_hyperparams.HIDDEN_DIM = 100\n",
        "compounding_hyperparams.EMBEDDING_DIM = 1\n",
        "\n",
        "#Numbers used in EfficientNet paper\n",
        "#depth coefficient * width coefficient^2 * embed_coefficient ^2 = 2\n",
        "depth_coefficient = 1.15\n",
        "width_coefficient = 1.1\n",
        "resolution_coefficient = 1.35\n",
        "\n",
        "current_n_layers= compounding_hyperparams.N_LAYERS\n",
        "current_hidden_dim = compounding_hyperparams.HIDDEN_DIM\n",
        "current_embedding_dim = compounding_hyperparams.EMBEDDING_DIM\n",
        "\n",
        "for i in range(1,10):\n",
        "  current_n_layers = current_n_layers * depth_coefficient\n",
        "  current_hidden_dim = current_hidden_dim * width_coefficient\n",
        "  current_embedding_dim = current_embedding_dim * resolution_coefficient\n",
        "  compounding_hyperparams.N_LAYERS = round(current_n_layers)\n",
        "  compounding_hyperparams.HIDDEN_DIM = round(current_hidden_dim)\n",
        "  compounding_hyperparams.EMBEDDING_DIM = round(current_embedding_dim)\n",
        "  print(f'N_LAYERS: {compounding_hyperparams.N_LAYERS }')\n",
        "  print(f'HIDDEN_DIM: {compounding_hyperparams.HIDDEN_DIM }')\n",
        "  print(f'EMBEDDING_DIM: {compounding_hyperparams.EMBEDDING_DIM }')\n",
        "  _ = train_and_test_model_with_hparams(compounding_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaODJwvk9pU-",
        "outputId": "cb9dd6e7-3582-4432-b8ab-99bab53a7e92"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N_LAYERS: 1\n",
            "HIDDEN_DIM: 110\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 110,775 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 45.21it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.25it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.687, train_acc: 0.546\n",
            "valid_loss: 0.658, valid_acc: 0.647\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 46.82it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.46it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.389, train_acc: 0.839\n",
            "valid_loss: 0.294, valid_acc: 0.877\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 46.69it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.42it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.172, train_acc: 0.939\n",
            "valid_loss: 0.297, valid_acc: 0.875\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 41.83it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.35it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.091, train_acc: 0.972\n",
            "valid_loss: 0.349, valid_acc: 0.886\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 46.63it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 82.54it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.050, train_acc: 0.986\n",
            "valid_loss: 0.407, valid_acc: 0.876\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 82.44it/s]\n",
            "test_loss: 0.439, test_acc: 0.865\n",
            "\n",
            "N_LAYERS: 1\n",
            "HIDDEN_DIM: 121\n",
            "EMBEDDING_DIM: 2\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 182,410 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.78it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 81.47it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.604, train_acc: 0.629\n",
            "valid_loss: 0.323, valid_acc: 0.866\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 46.22it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 83.76it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.249, train_acc: 0.904\n",
            "valid_loss: 0.327, valid_acc: 0.877\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 46.22it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.16it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.118, train_acc: 0.962\n",
            "valid_loss: 0.335, valid_acc: 0.883\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 45.59it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 79.17it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.065, train_acc: 0.980\n",
            "valid_loss: 0.406, valid_acc: 0.879\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 46.09it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.65it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.035, train_acc: 0.990\n",
            "valid_loss: 0.513, valid_acc: 0.873\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 81.32it/s]\n",
            "test_loss: 0.552, test_acc: 0.865\n",
            "\n",
            "N_LAYERS: 1\n",
            "HIDDEN_DIM: 133\n",
            "EMBEDDING_DIM: 2\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 194,818 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.53it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 80.50it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.616, train_acc: 0.629\n",
            "valid_loss: 0.334, valid_acc: 0.858\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.28it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 79.89it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.255, train_acc: 0.902\n",
            "valid_loss: 0.279, valid_acc: 0.893\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 45.65it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 83.13it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.132, train_acc: 0.956\n",
            "valid_loss: 0.313, valid_acc: 0.882\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 45.26it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 78.03it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.070, train_acc: 0.979\n",
            "valid_loss: 0.411, valid_acc: 0.879\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 45.67it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 81.04it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.036, train_acc: 0.990\n",
            "valid_loss: 0.487, valid_acc: 0.871\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 78.83it/s]\n",
            "test_loss: 0.535, test_acc: 0.866\n",
            "\n",
            "N_LAYERS: 1\n",
            "HIDDEN_DIM: 146\n",
            "EMBEDDING_DIM: 3\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 270,977 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.44it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 81.47it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.628, train_acc: 0.615\n",
            "valid_loss: 0.418, valid_acc: 0.819\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.42it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 78.40it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.313, train_acc: 0.880\n",
            "valid_loss: 0.373, valid_acc: 0.859\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.75it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 76.66it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.164, train_acc: 0.943\n",
            "valid_loss: 0.357, valid_acc: 0.873\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.63it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 81.41it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.084, train_acc: 0.975\n",
            "valid_loss: 0.394, valid_acc: 0.864\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 40.57it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 82.28it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.138, train_acc: 0.943\n",
            "valid_loss: 0.447, valid_acc: 0.824\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 78.58it/s]\n",
            "test_loss: 0.461, test_acc: 0.822\n",
            "\n",
            "N_LAYERS: 2\n",
            "HIDDEN_DIM: 161\n",
            "EMBEDDING_DIM: 4\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 559,860 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 30.16it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 65.21it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.652, train_acc: 0.604\n",
            "valid_loss: 0.542, valid_acc: 0.746\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 30.29it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 64.16it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.320, train_acc: 0.871\n",
            "valid_loss: 0.334, valid_acc: 0.861\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 28.85it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 65.29it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.168, train_acc: 0.941\n",
            "valid_loss: 0.378, valid_acc: 0.867\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 30.92it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 64.62it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.080, train_acc: 0.975\n",
            "valid_loss: 0.443, valid_acc: 0.857\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 30.39it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 65.04it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.039, train_acc: 0.989\n",
            "valid_loss: 0.521, valid_acc: 0.852\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 65.68it/s]\n",
            "test_loss: 0.557, test_acc: 0.842\n",
            "\n",
            "N_LAYERS: 2\n",
            "HIDDEN_DIM: 177\n",
            "EMBEDDING_DIM: 6\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 748,382 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 29.29it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 63.69it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.576, train_acc: 0.660\n",
            "valid_loss: 0.338, valid_acc: 0.863\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 29.45it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 62.06it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.229, train_acc: 0.912\n",
            "valid_loss: 0.299, valid_acc: 0.878\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 29.19it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 63.68it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.101, train_acc: 0.967\n",
            "valid_loss: 0.391, valid_acc: 0.876\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 29.31it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 63.83it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.050, train_acc: 0.985\n",
            "valid_loss: 0.490, valid_acc: 0.859\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 29.49it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 63.21it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.036, train_acc: 0.989\n",
            "valid_loss: 0.562, valid_acc: 0.861\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 52.86it/s]\n",
            "test_loss: 0.587, test_acc: 0.856\n",
            "\n",
            "N_LAYERS: 2\n",
            "HIDDEN_DIM: 195\n",
            "EMBEDDING_DIM: 8\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 952,716 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:13<00:00, 27.60it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 60.35it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.604, train_acc: 0.650\n",
            "valid_loss: 0.420, valid_acc: 0.810\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:13<00:00, 27.44it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 60.53it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.269, train_acc: 0.896\n",
            "valid_loss: 0.319, valid_acc: 0.869\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:13<00:00, 27.60it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 60.28it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.134, train_acc: 0.956\n",
            "valid_loss: 0.368, valid_acc: 0.872\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:13<00:00, 27.51it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 60.82it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.063, train_acc: 0.980\n",
            "valid_loss: 0.473, valid_acc: 0.852\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:13<00:00, 27.48it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 61.01it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.035, train_acc: 0.989\n",
            "valid_loss: 0.574, valid_acc: 0.851\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 60.48it/s]\n",
            "test_loss: 0.610, test_acc: 0.849\n",
            "\n",
            "N_LAYERS: 2\n",
            "HIDDEN_DIM: 214\n",
            "EMBEDDING_DIM: 11\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 1,231,985 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 25.87it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 56.19it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.602, train_acc: 0.640\n",
            "valid_loss: 0.386, valid_acc: 0.833\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 25.90it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 57.87it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.264, train_acc: 0.898\n",
            "valid_loss: 0.305, valid_acc: 0.876\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 26.03it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 55.85it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.124, train_acc: 0.958\n",
            "valid_loss: 0.408, valid_acc: 0.862\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 25.93it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 57.21it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.056, train_acc: 0.983\n",
            "valid_loss: 0.473, valid_acc: 0.863\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 24.77it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 57.54it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.027, train_acc: 0.992\n",
            "valid_loss: 0.675, valid_acc: 0.859\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 55.05it/s]\n",
            "test_loss: 0.698, test_acc: 0.856\n",
            "\n",
            "N_LAYERS: 2\n",
            "HIDDEN_DIM: 236\n",
            "EMBEDDING_DIM: 15\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 1,599,257 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:15<00:00, 24.11it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 53.80it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.580, train_acc: 0.657\n",
            "valid_loss: 0.366, valid_acc: 0.847\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 24.50it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 54.11it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.291, train_acc: 0.887\n",
            "valid_loss: 0.338, valid_acc: 0.867\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 24.37it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 53.83it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.131, train_acc: 0.957\n",
            "valid_loss: 0.384, valid_acc: 0.870\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 24.53it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 53.74it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.065, train_acc: 0.981\n",
            "valid_loss: 0.479, valid_acc: 0.858\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 24.48it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 51.81it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.038, train_acc: 0.988\n",
            "valid_loss: 0.512, valid_acc: 0.852\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:02<00:00, 51.65it/s]\n",
            "test_loss: 0.546, test_acc: 0.846\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform gri\n",
        "compounding_hyperparams = HyperParams()\n",
        "compounding_hyperparams.OPTIM = \"adam\"\n",
        "compounding_hyperparams.LR = 0.001\n",
        "compounding_hyperparams.N_LAYERS = 1\n",
        "compounding_hyperparams.HIDDEN_DIM = 175\n",
        "compounding_hyperparams.EMBEDDING_DIM = 1\n",
        "\n",
        "#Numbers used in EfficientNet paper\n",
        "#depth coefficient * width coefficient^2 * embed_coefficient ^2 = 2\n",
        "depth_coefficient = 1.1\n",
        "width_coefficient = 1.1\n",
        "resolution_coefficient = 1.25\n",
        "\n",
        "current_n_layers= compounding_hyperparams.N_LAYERS\n",
        "current_hidden_dim = compounding_hyperparams.HIDDEN_DIM\n",
        "current_embedding_dim = compounding_hyperparams.EMBEDDING_DIM\n",
        "\n",
        "for i in range(1,10):\n",
        "  current_n_layers = current_n_layers * depth_coefficient\n",
        "  current_hidden_dim = current_hidden_dim * width_coefficient\n",
        "  current_embedding_dim = current_embedding_dim * resolution_coefficient\n",
        "  compounding_hyperparams.N_LAYERS = round(current_n_layers)\n",
        "  compounding_hyperparams.HIDDEN_DIM = round(current_hidden_dim)\n",
        "  compounding_hyperparams.EMBEDDING_DIM = round(current_embedding_dim)\n",
        "  print(f'N_LAYERS: {compounding_hyperparams.N_LAYERS }')\n",
        "  print(f'HIDDEN_DIM: {compounding_hyperparams.HIDDEN_DIM }')\n",
        "  print(f'EMBEDDING_DIM: {compounding_hyperparams.EMBEDDING_DIM }')\n",
        "  _ = train_and_test_model_with_hparams(compounding_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoZeL0-VLy28",
        "outputId": "b7509a95-6368-4892-fccb-63cff03d0a93"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N_LAYERS: 1\n",
            "HIDDEN_DIM: 193\n",
            "EMBEDDING_DIM: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 212,533 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 43.01it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 74.83it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.626, train_acc: 0.612\n",
            "valid_loss: 0.381, valid_acc: 0.851\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 39.76it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 76.36it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.289, train_acc: 0.884\n",
            "valid_loss: 0.310, valid_acc: 0.867\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.14it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 76.66it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.162, train_acc: 0.945\n",
            "valid_loss: 0.330, valid_acc: 0.871\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 42.98it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 76.77it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.092, train_acc: 0.971\n",
            "valid_loss: 0.359, valid_acc: 0.860\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 43.32it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 76.15it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.092, train_acc: 0.968\n",
            "valid_loss: 0.433, valid_acc: 0.874\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 73.00it/s]\n",
            "test_loss: 0.484, test_acc: 0.867\n",
            "\n",
            "N_LAYERS: 1\n",
            "HIDDEN_DIM: 212\n",
            "EMBEDDING_DIM: 2\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 305,260 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 40.99it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 72.43it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.680, train_acc: 0.583\n",
            "valid_loss: 0.618, valid_acc: 0.727\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 41.15it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 71.77it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.327, train_acc: 0.871\n",
            "valid_loss: 0.281, valid_acc: 0.886\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 41.89it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 72.77it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.163, train_acc: 0.943\n",
            "valid_loss: 0.339, valid_acc: 0.854\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 41.83it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 70.93it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.086, train_acc: 0.973\n",
            "valid_loss: 0.375, valid_acc: 0.876\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 41.04it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 71.59it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.043, train_acc: 0.988\n",
            "valid_loss: 0.479, valid_acc: 0.872\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 70.94it/s]\n",
            "test_loss: 0.514, test_acc: 0.871\n",
            "\n",
            "N_LAYERS: 1\n",
            "HIDDEN_DIM: 233\n",
            "EMBEDDING_DIM: 2\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 343,018 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.57it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 69.47it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.681, train_acc: 0.560\n",
            "valid_loss: 0.646, valid_acc: 0.606\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.64it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 66.88it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.335, train_acc: 0.858\n",
            "valid_loss: 0.279, valid_acc: 0.888\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.99it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 69.67it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.145, train_acc: 0.950\n",
            "valid_loss: 0.321, valid_acc: 0.888\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.92it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 64.98it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.081, train_acc: 0.974\n",
            "valid_loss: 0.381, valid_acc: 0.876\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.91it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 68.61it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.049, train_acc: 0.985\n",
            "valid_loss: 0.464, valid_acc: 0.874\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 67.23it/s]\n",
            "test_loss: 0.501, test_acc: 0.863\n",
            "\n",
            "N_LAYERS: 1\n",
            "HIDDEN_DIM: 256\n",
            "EMBEDDING_DIM: 2\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 388,420 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 35.63it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 65.27it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.658, train_acc: 0.573\n",
            "valid_loss: 0.464, valid_acc: 0.788\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.67it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 68.73it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.284, train_acc: 0.887\n",
            "valid_loss: 0.287, valid_acc: 0.887\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.42it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 69.86it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.129, train_acc: 0.956\n",
            "valid_loss: 0.326, valid_acc: 0.883\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.15it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 69.01it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.064, train_acc: 0.981\n",
            "valid_loss: 0.414, valid_acc: 0.871\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.42it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 69.04it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.031, train_acc: 0.992\n",
            "valid_loss: 0.499, valid_acc: 0.866\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 67.52it/s]\n",
            "test_loss: 0.531, test_acc: 0.859\n",
            "\n",
            "N_LAYERS: 2\n",
            "HIDDEN_DIM: 282\n",
            "EMBEDDING_DIM: 3\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 1,145,249 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:18<00:00, 20.27it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 44.90it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.572, train_acc: 0.658\n",
            "valid_loss: 0.474, valid_acc: 0.785\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:18<00:00, 20.26it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 45.65it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.228, train_acc: 0.913\n",
            "valid_loss: 0.327, valid_acc: 0.872\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:18<00:00, 20.04it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 47.35it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.099, train_acc: 0.967\n",
            "valid_loss: 0.380, valid_acc: 0.854\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:17<00:00, 20.35it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 46.68it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.052, train_acc: 0.984\n",
            "valid_loss: 0.439, valid_acc: 0.869\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:17<00:00, 20.33it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 45.18it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.033, train_acc: 0.989\n",
            "valid_loss: 0.584, valid_acc: 0.853\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:02<00:00, 46.01it/s]\n",
            "test_loss: 0.619, test_acc: 0.848\n",
            "\n",
            "N_LAYERS: 2\n",
            "HIDDEN_DIM: 310\n",
            "EMBEDDING_DIM: 4\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 1,407,074 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:20<00:00, 17.95it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 41.58it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.566, train_acc: 0.671\n",
            "valid_loss: 0.334, valid_acc: 0.858\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:20<00:00, 18.04it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 41.61it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.233, train_acc: 0.911\n",
            "valid_loss: 0.300, valid_acc: 0.879\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:20<00:00, 18.09it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 41.97it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.099, train_acc: 0.967\n",
            "valid_loss: 0.385, valid_acc: 0.879\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:20<00:00, 18.11it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 42.67it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.069, train_acc: 0.977\n",
            "valid_loss: 0.481, valid_acc: 0.862\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:20<00:00, 18.09it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 41.85it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.036, train_acc: 0.989\n",
            "valid_loss: 0.600, valid_acc: 0.849\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:02<00:00, 41.61it/s]\n",
            "test_loss: 0.639, test_acc: 0.838\n",
            "\n",
            "N_LAYERS: 2\n",
            "HIDDEN_DIM: 341\n",
            "EMBEDDING_DIM: 5\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 1,712,497 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:23<00:00, 15.60it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 37.72it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.563, train_acc: 0.664\n",
            "valid_loss: 0.322, valid_acc: 0.872\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:23<00:00, 15.51it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 37.36it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.245, train_acc: 0.908\n",
            "valid_loss: 0.302, valid_acc: 0.885\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:23<00:00, 15.57it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 36.09it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.114, train_acc: 0.962\n",
            "valid_loss: 0.399, valid_acc: 0.859\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:23<00:00, 15.58it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 36.68it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.062, train_acc: 0.980\n",
            "valid_loss: 0.506, valid_acc: 0.866\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:23<00:00, 15.59it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 37.08it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.032, train_acc: 0.990\n",
            "valid_loss: 0.575, valid_acc: 0.865\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:02<00:00, 36.71it/s]\n",
            "test_loss: 0.637, test_acc: 0.857\n",
            "\n",
            "N_LAYERS: 2\n",
            "HIDDEN_DIM: 375\n",
            "EMBEDDING_DIM: 6\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 2,068,250 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:25<00:00, 14.24it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 35.04it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.610, train_acc: 0.634\n",
            "valid_loss: 0.420, valid_acc: 0.810\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:25<00:00, 14.24it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 35.27it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.285, train_acc: 0.889\n",
            "valid_loss: 0.320, valid_acc: 0.869\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:25<00:00, 14.20it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 34.95it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.140, train_acc: 0.951\n",
            "valid_loss: 0.350, valid_acc: 0.867\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:25<00:00, 14.27it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 35.78it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.065, train_acc: 0.980\n",
            "valid_loss: 0.461, valid_acc: 0.854\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:25<00:00, 14.22it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 35.42it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.035, train_acc: 0.990\n",
            "valid_loss: 0.663, valid_acc: 0.848\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:03<00:00, 34.95it/s]\n",
            "test_loss: 0.700, test_acc: 0.843\n",
            "\n",
            "N_LAYERS: 2\n",
            "HIDDEN_DIM: 413\n",
            "EMBEDDING_DIM: 7\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 2,491,659 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:30<00:00, 12.10it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 30.69it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.622, train_acc: 0.639\n",
            "valid_loss: 0.522, valid_acc: 0.794\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:30<00:00, 12.07it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 30.93it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.304, train_acc: 0.879\n",
            "valid_loss: 0.354, valid_acc: 0.848\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:30<00:00, 12.06it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 31.07it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.136, train_acc: 0.952\n",
            "valid_loss: 0.340, valid_acc: 0.867\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:30<00:00, 12.07it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 30.34it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.060, train_acc: 0.981\n",
            "valid_loss: 0.489, valid_acc: 0.858\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:30<00:00, 12.09it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 30.98it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.027, train_acc: 0.992\n",
            "valid_loss: 0.635, valid_acc: 0.856\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:03<00:00, 30.64it/s]\n",
            "test_loss: 0.667, test_acc: 0.851\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSQ-Js0mKxDn"
      },
      "source": [
        "### Lab 2 (g) Bi-Directional LSTM, using best architecture from (f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "O3E7_L_pKxDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce305262-70ff-4c1b-9ce3-50374884c385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60833\n",
            "The model has 68,815 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 40.43it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 78.28it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.620, train_acc: 0.608\n",
            "valid_loss: 0.361, valid_acc: 0.851\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 40.79it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 79.21it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.271, train_acc: 0.897\n",
            "valid_loss: 0.273, valid_acc: 0.890\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 41.14it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 79.17it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.151, train_acc: 0.950\n",
            "valid_loss: 0.291, valid_acc: 0.889\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 40.76it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 78.77it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.090, train_acc: 0.973\n",
            "valid_loss: 0.322, valid_acc: 0.894\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 40.89it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 79.12it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.051, train_acc: 0.987\n",
            "valid_loss: 0.398, valid_acc: 0.886\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 78.28it/s]\n",
            "test_loss: 0.449, test_acc: 0.878\n"
          ]
        }
      ],
      "source": [
        "adam_birectional_hyperparams = HyperParams()\n",
        "adam_birectional_hyperparams.OPTIM = \"adam\"\n",
        "adam_birectional_hyperparams.LR = 0.001\n",
        "adam_birectional_hyperparams.N_LAYERS = 1\n",
        "adam_birectional_hyperparams.HIDDEN_DIM = 30\n",
        "adam_birectional_hyperparams.EMBEDDING_DIM = 1\n",
        "\n",
        "adam_birectional_hyperparams.BIDIRECTIONAL = True\n",
        "_ = train_and_test_model_with_hparams(adam_birectional_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Stub_aoCrqzc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}